#!/usr/bin/python
# -*- coding: utf-8 -*-
'''
This script creates poem-like paragraphs levevraging pre-trained 
casual language models.

'''
from __future__ import print_function
import logging, os, argparse
from transformers import BertTokenizerFast, AutoModelForCausalLM
import pandas as pd
import numpy as np
import jieba
import pickle

def get_word_vector(word, word_embedding):
    ''' Look-up the word vector from the given word-embedding. '''
    if word_embedding is None:
        word_embedding = load_word_embedding()
    try:
        vec = word_embedding[word]
    except KeyError as e:
        print(e)
        vec = np.zeros((300))
    return(vec)


def evaluate_tokens(tokens, word_embedding):
    n_token = len(tokens)
    vec = np.zeros((300))
    for i in range(n_token):
        vec = vec + get_word_vector(tokens[i])
    vec = vec/n_token
    return(vec)


def generate_starting_sentence(input):
    ''' Use the seeding information to create the starting sentence. '''
    sentence = input
    return(sentence)


def decode_generated_ids(generated, tokenizer):
    ''' Decode the ids generated by the language model. '''
    output=[]
    for i in range(10):
        text = tokenizer.decode(generated[i], skip_special_tokens= True)    # Decode the generated text
        text = text.replace(' ','')                                         # Remove spaces between tokens
        text = text.replace(',','，')
        output.append(text)
    return(output)

def generate_new_sentences(input, tokenizer, model):
    ''' Generate new sentences with specified model and tokenizer. '''
    # Parse seeding string
    input_ids = tokenizer.encode(input, return_tensors='pt')
    # Generate text
    generated = model.generate(input_ids, 
                            max_length=50,  
                            num_return_sequences=10,
                            no_repeat_ngram_size=2,
                            repetition_penalty=1.05,
                            length_penalty=0.8,
                            top_p=0.8,
                            temperature=.8,
                            top_k=128,
                            do_sample=True,
                            early_stopping=True)
    # Decode
    output = decode_generated_ids(generated, tokenizer)
    # Done
    return(output)


def postprocess_generated_sentences(sentences, seed_sentence, vdict):
    ''' Post-process the generated paragraph. '''
    # Define sentence-break symbols
    bs = ['，','。','；','！','？']
    # Loop through all generated snetences
    svecs = []
    stokens = []
    for s in sentences:
        temp = s.replace(seed_sentence, '')     # Remove the seed sentence
        tokens = list(jieba.cut(temp))          # Tokenize the sentence with jieba
        # Looking for sentence-break symbols
        idxs = [i for i, x in enumerate(tokens) if x in bs]
        print(len(idxs))
        if len(idxs)>1:                         # Keep tokens before the fisrt break
            tokens = tokens[idxs[0]+1:idxs[1]]
        elif len(idxs)>0:
            tokens = tokens[:idxs[0]]
        else:
            tokens = tokens
        svec = evaluate_tokens(tokens, vdict)   # Calculate the word-embedding vectors of the tokens
        svecs.append(svec)
        stokens.append(tokens)
    #
    return({'sentences':stokens,'wvectors':svecs})


def select_next_sentence(candidates, seed_vec):
    ''' Select the best candidate. '''
    scores = []
    for i in range(len(candidates['sentences'])):
        print(candidates['sentences'][i])
        score = np.dot(seed_vec, candidates['wvectors'][i])
        print(score)
        scores.append(score)
    return({'sentence':candidates['sentences'][scores.index(max(scores))], 
            'vector':candidates['wvectors'][scores.index(max(scores))]})



def generate_poem(seed_sentence, total_lines, we):
    ''' Generate a poem starting with seed_sentence '''
    output = []
    output.append(seed_sentence)
    seed_vec = evaluate_tokens(list(jieba.cut(seed_sentence)), we)
    # Generate followed-up sentences
    for i in range(total_lines):
        generated = generate_new_sentences(seed_sentence, tokenizer, model)
        candidates = postprocess_generated_sentences(generated, seed_sentence, we)
        selected = select_next_sentence(candidates, seed_vec)
        output.append(''.join(selected['sentence']))
        seed_vec = selected['vector']
        seed_sentence = ''.join(selected['sentence'])
    # done
    poem = '\n'.join(output)
    return(poem)

#-----------------------------------------------------------------------
def main():
    '''    '''
    # Configure Argument Parser
    parser = argparse.ArgumentParser(description='Retrieve DBZ data for further processing.')
    parser.add_argument('--config_file', '-c', default=None, help='the configuration file.')
    parser.add_argument('--logfile', '-l', default=None, help='the log file.')
    parser.add_argument('--random_seed', '-r', default=None, help='the seed for random numbers.')
    args = parser.parse_args()
    # Set up logging
    if not args.logfile is None:
        logging.basicConfig(level=logging.DEBUG, filename=args.logfile, filemode='w')
    else:
        logging.basicConfig(level=logging.DEBUG)
    logging.debug(args)
    # Default configuration
    TOKENIZER_PATH = '../data/tokenizer_bert_base_chinese'
    MODEL_PATH = '../model/ckip'
    MODEL_TF = True
    WORD_EMBEDDING_PATH = 'D:/workspace/word_embedding/zh_wiki_word2vec_300.pkl'
    GEN_PARAMS = {
        "max_length": 30,  
        "num_return_sequences": 10,
        "no_repeat_ngram_size": 2,
        "repetition_penalty": 1.5,
        "length_penalty": 1.0,
        "top_p": 0.92,
        "temperature": 0.85,
        "top_k": 16
    }
    # Load configuration file if specified
    if not args.config_file is None:
        conf = json.load(open(args.config_file, 'r'))
        TOKENIZER_PATH = conf['tokenizer_path']
        MODEL_PATH = conf['model_path']
        MODEL_TF = conf['model_tf']
        WORD_EMBEDDING_PATH = conf['word_embedding_path']
        GEN_PARAMS = conf['gen_params']
    # Prompt the setting
    logging.info()
    # Initialize tokenizer and model
    tokenizer = BertTokenizerFast.from_pretrained(TOKENIZER_PATH, clean_text=True)
    model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, from_tf=MODEL_TF)
    # Load word embeddings
    with open(WORD_EMBEDDING_PATH, 'rb') as f:
        we = pickle.load(f)
    # Generate random numbers
    np.random.seed(random_seed)                 # Set random-state
    total_lines = np.random.randint( 5,15)      # Define the total lines
    logging.info('To generate '+str(total_lines)+' sentences in total.')
    # Generate starting sentence
    output = []
    seed_sentence = generate_starting_sentence(args.input)
    logging.info('Sentence 0: '+seed_sentence)
    output.append(seed_sentence)
    seed_vec = evaluate_tokens(list(jieba.cut(seed_sentence)), we)
    # Generate followed-up sentences
    for i in range(total_lines):
        generated = generate_new_sentences(seed_sentence, tokenizer, model)
        candidates = postprocess_generated_sentences(generated, seed_sentence, we)
        selected = select_next_sentence(candidates, seed_vec)
        output.append(''.join(selected['sentence']))
        seed_vec = selected['vector']
        seed_sentence = selected['sentence']
    # done
    print(output)
    return(0)

#==========
# Script
#==========
if __name__ == "__main__":
    main()