#!/usr/bin/python
# -*- coding: utf-8 -*-
'''
This script creates poem-like paragraphs starting with PTTGOSSIP data.
'''
from __future__ import print_function
import logging, os, argparse
from transformers import BertTokenizerFast, AutoModelForCausalLM
from sentence_transformers import SentenceTransformer, util
import pandas as pd
import numpy as np
import json
import requests
import re

STOP_WORDS = ['記者','即時報導','ETtoday','新聞文章','ettoday','/','三立','東森']
IGNORED_WORDS = ['台北','台灣']
MIN_LINES = 16

def contains_stopwords(string, stop_words):
    ''' Check if the string contains any of the stop_word'''
    for w in stop_words:
        if w in string:
            return(True)
    return(False)

def generate_starting_sentence(candidates, stop_words=STOP_WORDS):
    ''' Use the cadidates to create the starting sentence. '''
    filtered = []
    # Post-processing candidates
    for i in range(candidates.shape[0]):
        c = candidates['sentence'].iloc[i]
        if not contains_stopwords(c, STOP_WORDS):
            filtered.append(c)
    # Random sample
    samples = np.random.choice(filtered, size=1, replace=False)
    sentence = ''.join(list(samples))
    logging.debug(sentence)
    return(sentence)


def evaluate_tokens(tokens, model):
    embeddings = model.encode(tokens)
    return(embeddings)


def decode_generated_ids(generated, tokenizer):
    ''' Decode the ids generated by the language model. '''
    output=[]
    for i in range(10):
        text = tokenizer.decode(generated[i], skip_special_tokens= True)    # Decode the generated text
        text = text.replace(' ','')                                         # Remove spaces between tokens
        text = text.replace(',','，')
        output.append(text)
    return(output)

def generate_new_sentences(input, tokenizer, model, params):
    ''' Generate new sentences with specified model and tokenizer. '''
    # Parse seeding string
    input_ids = tokenizer.encode(input, return_tensors='pt')
    # Generate text
    generated = model.generate(input_ids, 
                            max_length=params['max_length'],  
                            num_return_sequences=params['num_return_sequences'],
                            no_repeat_ngram_size=params['no_repeat_ngram_size'],
                            repetition_penalty=params['repetition_penalty'],
                            length_penalty=params['length_penalty'],
                            top_p=params['top_p'],
                            temperature=params['temperature'],
                            top_k=params['top_k'],
                            do_sample=True,
                            early_stopping=True)
    # Decode
    output = decode_generated_ids(generated, tokenizer)
    # Done
    return(output)


def postprocess_generated_sentences(sentences, history_sentences, sent_transformer):
    ''' Post-process the generated paragraph. '''
    # Define sentence-break symbols
    bs = ['，','。','；','！','？','「','」','：']   # Separators
    bsre = '|'.join(bs)                     # Separators for Regular Expression
    seed_sentence = history_sentences[-1]
    # Loop through all generated snetences
    svecs = []
    stokens = []
    for s in sentences:
        temp = s.replace(seed_sentence, '')     # Remove the seed sentence
        # Split the paragraph with break-symbols
        templist = re.split(bsre, temp)
        for tokens in templist:
            if len(tokens.strip())<=3:
                logging.debug("Short sentence, skip.")
                continue            
            if tokens.strip()=='':
                logging.debug("Empty sentence, skip.")
                continue
            if tokens in history_sentences:
                logging.debug("Generated senytence already existed, skip.")
                continue
            svec = sent_transformer.encode(tokens)   # Calculate the sentence-embedding vectors of the tokens
            svecs.append({'sentence':tokens, 'embedding':svec})
        # # Looking for sentence-break symbols
        # idxs = [i for i, x in enumerate(temp) if x in bs]
        # if len(idxs)>1:                         # Keep tokens before the fisrt break
        #     tokens = temp[idxs[0]+1:idxs[1]]
        #     logging.debug("Take the segment between the 1st and 2nd punchuations. "+str(len(idxs)))
        #     if tokens.strip()=='':
        #         logging.debug("Empty sentence, skip.")
        #         continue
        #     if tokens in history_sentences:
        #         logging.debug("Generated senytence already existed, skip.")
        #         continue
        # #elif len(idxs)>0:
        # #    tokens = tokens[:idxs[0]]
        # else:                                   # Skip empty sentence
        #     logging.debug('The generated sentence is too short, skip it: '+s)
        #     continue
        # svec = sent_transformer.encode(tokens)   # Calculate the sentence-embedding vectors of the tokens
        # svecs.append({'sentence':tokens, 'embedding':svec})
    #
    return(svecs)


def select_next_sentence(candidates, embeddings, back_length=3):
    ''' Select the best candidate. '''
    scores = []
    for i in range(len(candidates)):
        score = 0
        logging.debug(candidates[i]['sentence'])
        emb_length = len(embeddings)
        if emb_length<back_length:
            seed_vec = embeddings[-1]
            score += np.dot(seed_vec, candidates[i]['embedding'])
        else:
            for j in range(emb_length, emb_length-back_length, -1):
                seed_vec = embeddings[j-1]
                weight = j-emb_length+back_length
                weight_sign = (weight%2)==1 and 1 or -1
                #logging.debug([j, weight, weight_sign])
                score += np.dot(seed_vec, candidates[i]['embedding'])*(weight)*(weight_sign)
        logging.debug(score)
        scores.append(score)
    return(candidates[scores.index(max(scores))])

def postprocess_poem(sentences):
    ''' Post-process the generated poem. '''
    poem = {'title': sentences[0],
            'content': '\n'.join(sentences[1:])}
    return(poem)


def shuffle_sentence(sentence, new_length=0):
    ''' Randomly shuffle a sring. '''
    import numpy as np
    slist = list(sentence)
    if new_length==0:
        np.random.shuffle(slist)
    else:
        slist = np.random.choice(slist, new_length)
    new_sentence = ''.join(slist)
    return(new_sentence)


def generate_poem(seed_sentence, model, tokenizer, st, params):
    ''' Generate a poem starting with seed_sentence '''
    output = []
    embeddings = []
    output.append(seed_sentence)
    seed_vec = evaluate_tokens(seed_sentence, st)
    embeddings.append(seed_vec)
    # Generate followed-up sentences
    #for i in range(params['total_lines']):
    while len(output)<params['total_lines']:
        generated = generate_new_sentences(seed_sentence, tokenizer, model, params)
        candidates = postprocess_generated_sentences(generated, output, st)
        if len(candidates)>0:
            selected = select_next_sentence(candidates, embeddings)
        else:
            seed_sentence = shuffle_sentence(seed_sentence, new_length=7)
            logging.debug('Generated sentences are not qualified, shuffle the seeding sentence to: '+seed_sentence)
            continue
        output.append(selected['sentence'])
        embeddings.append(selected['embedding'])
        seed_vec = selected['embedding']
        seed_sentence = selected['sentence']
    # done
    poem = postprocess_poem(output)
    return(poem)

#-----------------------------------------------------------------------
def main():
    '''    '''
    # Configure Argument Parser
    parser = argparse.ArgumentParser(description='Generate a poem with specified languag model and starting information.')
    parser.add_argument('--input', '-i', help='the input information for the starting sentence.')
    parser.add_argument('--config_file', '-c', default=None, help='the configuration file.')
    parser.add_argument('--logfile', '-l', default=None, help='the log file.')
    parser.add_argument('--random_seed', '-r', default=None, type=int, help='the seed for random numbers.')
    args = parser.parse_args()
    # Set up logging
    if not args.logfile is None:
        logging.basicConfig(level=logging.DEBUG, filename=args.logfile, filemode='w')
    else:
        logging.basicConfig(level=logging.DEBUG)
    # Prompt the setting
    logging.debug(args)
    # Default configuration
    TOKENIZER_PATH = '../model/test'
    MODEL_PATH = '../model/test'
    MODEL_TF = True
    WORD_EMBEDDING_PATH = 'distiluse-base-multilingual-cased-v2'
    GEN_PARAMS = {
        "max_length": 30,  
        "num_return_sequences": 10,
        "no_repeat_ngram_size": 2,
        "repetition_penalty": 1.5,
        "length_penalty": 1.0,
        "top_p": 0.92,
        "temperature": 0.85,
        "top_k": 16
    }
    # Load configuration file if specified
    if not args.config_file is None:
        conf = json.load(open(args.config_file, 'r'))
        TOKENIZER_PATH = conf['tokenizer_path']
        MODEL_PATH = conf['model_path']
        MODEL_TF = conf['model_tf']
        WORD_EMBEDDING_PATH = conf['word_embedding_path']
        GEN_PARAMS = conf['gen_params']
    # Initialize tokenizer and model
    logging.info("Loading tokenizer from "+TOKENIZER_PATH)
    tokenizer = BertTokenizerFast.from_pretrained(TOKENIZER_PATH)
    logging.info("Loading language model from "+MODEL_PATH)
    model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, from_tf=eval(MODEL_TF))
    # Load word embeddings
    logging.info("Loading pre-trained sentence transformer from "+WORD_EMBEDDING_PATH)
    st = SentenceTransformer(WORD_EMBEDDING_PATH)
    # Generate random numbers
    np.random.seed(args.random_seed)                        # Set random-state
    total_lines = np.random.randint(MIN_LINES,MIN_LINES+8)  # Define the total lines
    GEN_PARAMS['total_lines'] = total_lines
    # Generate starting sentence
    logging.info('Generating the starting sentence...')
    seed_sentences = pd.read_csv('seed_sentences.csv')
    seed_sentence = generate_starting_sentence(seed_sentences)
    logging.info('To generate '+str(total_lines)+' sentences starting with ['+seed_sentence+']')
    # Generate followed-up sentences
    output = generate_poem(seed_sentence, model, tokenizer, st, GEN_PARAMS)
    # Generate output
    poem_text = output['title']+'\n\n'+output['content']
    poem_embedding = st.encode(output['content'].replace('\n',','))
    print('\n'+poem_text)
    with open('poem.txt', 'w') as f:
        f.write(poem_text)
    np.save('poem.npy', np.array(poem_embedding))
    return(0)

#==========
# Script
#==========
if __name__ == "__main__":
    main()
