{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Your Own GPT-2\n",
    "\n",
    "## 2. Fine Tuning the Language Model\n",
    "\n",
    "在[上一篇]()裡我們用 HuggingFace `transformers` 套件裡預先訓練好的標記化工具（tokenizer）跟語言模型（language model），來自動生成新的文本，接下來要嘗試進一步用自己準備的語料來調整預先訓練的語言模型。\n",
    "\n",
    "### 參考資料：\n",
    "- [Natural Language Generation Part 2: GPT2 and Huggingface](https://towardsdatascience.com/natural-language-generation-part-2-gpt-2-and-huggingface-f3acb35bc86a)\n",
    "\n",
    "- [(transformers-document) Training and fine-tuning](https://huggingface.co/transformers/training.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2LMHeadModel: ['transformer.h.6.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing TFGPT2LMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFGPT2LMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[ 101  791 1921 1921 3706 2523 1962  102]], shape=(1, 8), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# setup imports to use the model\n",
    "from transformers import TFGPT2LMHeadModel\n",
    "from transformers import GPT2Tokenizer, BertTokenizer\n",
    "\n",
    "model = TFGPT2LMHeadModel.from_pretrained('ckiplab/gpt2-base-chinese', from_pt=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "input_ids = tokenizer.encode(\"今天天氣很好\", return_tensors='tf')\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們可以把預訓練好的模型先存起來，作為後續 fine-tune 的基礎。\n",
    "\n",
    "> model.save_pretrained('../data/mygpt2/')\n",
    "\n",
    "\n",
    "## 把語料轉換成訓練用的格式\n",
    "\n",
    "接下來，我們要把訓練用的語料，轉換成 `TFGPT2LMHeadModel()` 使用的格式。\n",
    "\n",
    "### 1. 製作語料文本的檔案清單"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1274\n",
      "['D:\\\\data\\\\corpus\\\\wiki_zh\\\\AA\\\\wiki_00', 'D:\\\\data\\\\corpus\\\\wiki_zh\\\\AA\\\\wiki_01', 'D:\\\\data\\\\corpus\\\\wiki_zh\\\\AA\\\\wiki_02', 'D:\\\\data\\\\corpus\\\\wiki_zh\\\\AA\\\\wiki_03', 'D:\\\\data\\\\corpus\\\\wiki_zh\\\\AA\\\\wiki_04']\n"
     ]
    }
   ],
   "source": [
    "# Get all files in the corpus directory\n",
    "def list_corpus_files(corpus_path, prefix='wiki_'):\n",
    "    import os\n",
    "    # \n",
    "    flist = []\n",
    "    for dirPath, dirNames, fileNames in os.walk(corpus_path):\n",
    "        for f in fileNames:\n",
    "            if f.startswith(prefix):\n",
    "                flist.append(os.path.join(dirPath, f))\n",
    "    return(flist)\n",
    "\n",
    "# Test function with the wiki_zh corpus\n",
    "wikifiles = list_corpus_files('D:\\data\\corpus\\wiki_zh')\n",
    "\n",
    "print(len(wikifiles))\n",
    "print(wikifiles[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 讀取單一語料檔案的工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n",
      "年表列表\n",
      "\n",
      "這是年表列表，年表或歷史年表（Timeline），又稱時間表、時間軸，是將有關歷史的資料依時間或年份先後排列而成的列表，即將一件發生在某年某月某日的歷史事件，以最簡單的形式，重點地記錄下來\n"
     ]
    }
   ],
   "source": [
    "# Read specified json file\n",
    "def read_json_corpus(corpus_file, to_zhtw=True):\n",
    "    import json\n",
    "    import opencc\n",
    "    converter = opencc.OpenCC('s2tw.json')          # To Taiwan Chinese\n",
    "    data = []\n",
    "    with open(corpus_file, 'r', encoding='utf8') as f:\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            if to_zhtw:\n",
    "                line = converter.convert(line)\n",
    "            data.append(json.loads(line))\n",
    "            line = f.readline()\n",
    "    return(data)\n",
    "\n",
    "# Test \n",
    "data = read_json_corpus(wikifiles[1])\n",
    "print(len(data))\n",
    "print(data[5]['text'][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 將語料轉換成標記化資料 (tokenized data)\n",
    "\n",
    "我們建好可以讀取大量文本的工具函數，在我們參考 [`gpt2-chinese`](https://github.com/Morizeyao/GPT2-Chinese/) 的 [`train.py`](https://github.com/Morizeyao/GPT2-Chinese/blob/master/train.py) 當中，有一個將大量文本轉換成「標記化資料 (tokenized data)」的函數，進行的工作如下：\n",
    "\n",
    "1. 將「分行符號」（`\\n`）轉換成 BertTokenizer 的段落符號（`[SEP]`）   \n",
    "2. 重組以「文件」為單位的語料：\n",
    "    1. 指定最後要分割成的檔案數量 `num_pieces`，將文件數量平均分配到每個 piece 裡\n",
    "    2. 將長度超過 `min_length` 的文件透過指定的 tokenizer (here, we use BertTokenizer) 轉換成向量，並在文件前後分別加上`[MASK]`和`[CLS]`的標記。\n",
    "    3. 以 piece 為單位儲存。\n",
    "\n",
    "基本上這個函數是將文件語料轉換成 token vector 的工具，不見得一定要這麼使用。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_files(lines, tokenized_data_path, num_pieces, full_tokenizer, min_length):\n",
    "    import os, tqdm\n",
    "    # Process raw strings \n",
    "    print('reading lines')\n",
    "    lines = [line.replace('\\n', ' [SEP] ') for line in lines]  # 用[SEP]表示换行, 段落之间使用SEP表示段落结束\n",
    "    all_len = len(lines)\n",
    "    # Prepare output\n",
    "    if not os.path.exists(tokenized_data_path):\n",
    "        print('creating path for tokenized data: '+tokenized_data_path)\n",
    "        os.mkdir(tokenized_data_path)\n",
    "    for i in tqdm.tqdm(range(num_pieces)):\n",
    "        sublines = lines[(all_len // num_pieces * i):(all_len // num_pieces * (i + 1))]\n",
    "        if i == num_pieces - 1:\n",
    "            sublines.extend(lines[all_len // num_pieces * (i + 1):])  # 把尾部例子添加到最后一个 piece\n",
    "        sublines = [full_tokenizer.tokenize(line) for line in sublines if\n",
    "                    len(line) > min_length]  # 只考虑长度超过 min_length 的句子\n",
    "        sublines = [full_tokenizer.convert_tokens_to_ids(line) for line in sublines]\n",
    "        full_line = []\n",
    "        for subline in sublines:\n",
    "            full_line.append(full_tokenizer.convert_tokens_to_ids('[MASK]'))  # 文章开头添加MASK表示文章开始\n",
    "            full_line.extend(subline)\n",
    "            full_line.append(full_tokenizer.convert_tokens_to_ids('[CLS]'))  # 文章之间添加CLS表示文章结束\n",
    "        with open(tokenized_data_path + 'tokenized_train_{}.txt'.format(i), 'w') as f:\n",
    "            for id in full_line:\n",
    "                f.write(str(id) + ' ')\n",
    "    print('finish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**製作以「文件」為單位的資料集**\n",
    "\n",
    "我們利用前面讀取語料檔案的工具，來建立以「文件」為單位的資料集。讓我們先讀取10個檔案作測試，我們不需要語料當中的其他欄位，只需要'text'即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:05<00:00,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "885\n",
      "885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Fetch content from all wikifiles\n",
    "import tqdm\n",
    "data = []\n",
    "for i in tqdm.tqdm(range(len(wikifiles[:10]))):\n",
    "    data+=read_json_corpus(wikifiles[i])\n",
    "    \n",
    "print(len(data))\n",
    "corpus_text = []\n",
    "for article in data:\n",
    "    corpus_text.append(article['text'])\n",
    "    \n",
    "print(len(corpus_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**分段測試函數功能**\n",
    "\n",
    "讀取的10個檔案中總共有885篇文件，由於上面的函數比較長，我們用分段測試來理解它的功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "政治學\n",
      "\n",
      "政治學是一門以研究政治行為、政治體制以及政治相關領域為主的社會科學學科。在西方，政治學在學術領域裡的研究也被稱為政治研究、政治科學、或只有政治兩字。政治學意味著在學術上的研究領域，政治研究則\n",
      "數學 [SEP]  [SEP] 數學是利用符號語言研究數量、結構、變化以及空間等概念的一門學科，從某種角度看屬於形式科學的一種。數學透過抽象化和邏輯推理的使用，由計數、計算、量度和對物體形狀及運動的觀\n",
      "885\n"
     ]
    }
   ],
   "source": [
    "print(corpus_text[10][:100])\n",
    "lines = [line.replace('\\n', ' [SEP] ') for line in corpus_text]  # 用[SEP]表示换行, 段落之间使用SEP表示段落结束\n",
    "print(lines[0][:100])\n",
    "all_len = len(lines)\n",
    "print(all_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下來這段是主迴圈，基本上就是把 article-based 單位換成 piece-based 的單位。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 88\n",
      "88\n",
      "88 176\n",
      "87\n",
      "176 264\n",
      "87\n",
      "264 352\n",
      "87\n",
      "352 440\n",
      "88\n",
      "440 528\n",
      "86\n",
      "528 616\n",
      "86\n",
      "616 704\n",
      "88\n",
      "704 792\n",
      "32\n",
      "792 880\n",
      "92\n"
     ]
    }
   ],
   "source": [
    "num_pieces = 10\n",
    "min_length = 30\n",
    "for i in range(num_pieces):\n",
    "    idx1 = all_len // num_pieces * i\n",
    "    idx2 = all_len // num_pieces * (i + 1)\n",
    "    print(idx1, idx2)\n",
    "    sublines = lines[(all_len // num_pieces * i):(all_len // num_pieces * (i + 1))]\n",
    "    if i == num_pieces - 1:\n",
    "        sublines.extend(lines[all_len // num_pieces * (i + 1):])  # 把尾部例子添加到最后一个 piece\n",
    "    sublines = [tokenizer.tokenize(line) for line in sublines if len(line) > min_length]  # 只考虑长度超过 min_length 的句子\n",
    "    sublines = [tokenizer.convert_tokens_to_ids(line) for line in sublines]\n",
    "    print(len(sublines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:19<00:00,  5.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "build_files(corpus_text, tokenized_data_path='../data/tokenized_data/', num_pieces=100,\n",
    "                    full_tokenizer=tokenizer, min_length=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 建立我們的訓練用標記化語料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all files in the corpus directory\n",
    "def list_corpus_files(corpus_path, prefix='wiki_'):\n",
    "    import os\n",
    "    # \n",
    "    flist = []\n",
    "    for dirPath, dirNames, fileNames in os.walk(corpus_path):\n",
    "        for f in fileNames:\n",
    "            if f.startswith(prefix):\n",
    "                flist.append(os.path.join(dirPath, f))\n",
    "    return(flist)\n",
    "\n",
    "# Read specified json file\n",
    "def read_json_corpus(corpus_file, to_zhtw=True):\n",
    "    import json\n",
    "    import opencc\n",
    "    converter = opencc.OpenCC('s2tw.json')          # To Taiwan Chinese\n",
    "    data = []\n",
    "    with open(corpus_file, 'r', encoding='utf8') as f:\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            if to_zhtw:\n",
    "                line = converter.convert(line)\n",
    "            data.append(json.loads(line))\n",
    "            line = f.readline()\n",
    "    return(data)\n",
    "\n",
    "def build_files(lines, tokenized_data_path, num_pieces, full_tokenizer, min_length):\n",
    "    import os, tqdm\n",
    "    # Process raw strings \n",
    "    print('reading lines')\n",
    "    lines = [line.replace('\\n', ' [SEP] ') for line in lines]  # 用[SEP]表示换行, 段落之间使用SEP表示段落结束\n",
    "    all_len = len(lines)\n",
    "    # Prepare output\n",
    "    if not os.path.exists(tokenized_data_path):\n",
    "        print('creating path for tokenized data: '+tokenized_data_path)\n",
    "        os.mkdir(tokenized_data_path)\n",
    "    for i in tqdm.tqdm(range(num_pieces)):\n",
    "        sublines = lines[(all_len // num_pieces * i):(all_len // num_pieces * (i + 1))]\n",
    "        if i == num_pieces - 1:\n",
    "            sublines.extend(lines[all_len // num_pieces * (i + 1):])  # 把尾部例子添加到最后一个 piece\n",
    "        sublines = [full_tokenizer.tokenize(line) for line in sublines if\n",
    "                    len(line) > min_length]  # 只考虑长度超过 min_length 的句子\n",
    "        sublines = [full_tokenizer.convert_tokens_to_ids(line) for line in sublines]\n",
    "        full_line = []\n",
    "        for subline in sublines:\n",
    "            full_line.append(full_tokenizer.convert_tokens_to_ids('[MASK]'))  # 文章开头添加MASK表示文章开始\n",
    "            full_line.extend(subline)\n",
    "            full_line.append(full_tokenizer.convert_tokens_to_ids('[CLS]'))  # 文章之间添加CLS表示文章结束\n",
    "        with open(tokenized_data_path + 'tokenized_train_{}.txt'.format(i), 'w') as f:\n",
    "            for id in full_line:\n",
    "                f.write(str(id) + ' ')\n",
    "    print('finish')\n",
    "\n",
    "def build_tokenized_corpus(corpus_path, \n",
    "                           tokenizer, \n",
    "                           output_path='../data/tokenized_data/', \n",
    "                           output_pieces=100, \n",
    "                           min_length=10):\n",
    "    import tqdm\n",
    "    # List all corpus files\n",
    "    corpusfiles = list_corpus_files(corpus_path)\n",
    "    print('Number of files: '+str(len(corpusfiles)))\n",
    "    # Read and combine corpus\n",
    "    print('Reading files... ')\n",
    "    data = []\n",
    "    for i in tqdm.tqdm(range(len(corpusfiles))):\n",
    "        data+=read_json_corpus(corpusfiles[i])\n",
    "    # Convert file-based corpus to article-based\n",
    "    corpus_text = []\n",
    "    for article in data:\n",
    "        corpus_text.append(article['text'])\n",
    "    #\n",
    "    build_files(corpus_text, tokenized_data_path='../data/tokenized_data/', num_pieces=output_pieces,\n",
    "                    full_tokenizer=tokenizer, min_length=min_length)\n",
    "    \n",
    "    #\n",
    "    return(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                         | 0/1274 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 1274\n",
      "Reading files... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1274/1274 [12:22<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/256 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating path for tokenized data: ../data/tokenized_data/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▉                                                                               | 3/256 [02:37<3:41:56, 52.64s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-27e095f06c0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bert-base-chinese\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbuild_tokenized_corpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'D:\\data\\corpus\\wiki_zh'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_pieces\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-865221ba1ab8>\u001b[0m in \u001b[0;36mbuild_tokenized_corpus\u001b[1;34m(corpus_path, tokenizer, output_path, output_pieces, min_length)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     build_files(corpus_text, tokenized_data_path='../data/tokenized_data/', num_pieces=output_pieces,\n\u001b[1;32m---> 74\u001b[1;33m                     full_tokenizer=tokenizer, min_length=min_length)\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-865221ba1ab8>\u001b[0m in \u001b[0;36mbuild_files\u001b[1;34m(lines, tokenized_data_path, num_pieces, full_tokenizer, min_length)\u001b[0m\n\u001b[0;32m     41\u001b[0m         sublines = [full_tokenizer.tokenize(line) for line in sublines if\n\u001b[0;32m     42\u001b[0m                     len(line) > min_length]  # 只考虑长度超过 min_length 的句子\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0msublines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfull_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msublines\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[0mfull_line\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0msubline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msublines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-865221ba1ab8>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     41\u001b[0m         sublines = [full_tokenizer.tokenize(line) for line in sublines if\n\u001b[0;32m     42\u001b[0m                     len(line) > min_length]  # 只考虑长度超过 min_length 的句子\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0msublines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfull_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msublines\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[0mfull_line\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0msubline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msublines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36mconvert_tokens_to_ids\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    371\u001b[0m         \u001b[0mids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 373\u001b[1;33m             \u001b[0mids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_token_to_id_with_added_voc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    374\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "build_tokenized_corpus(corpus_path='D:\\data\\corpus\\wiki_zh', tokenizer=tokenizer, output_pieces=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用 Model 類別直接訓練\n",
    "\n",
    "我們先讀取預訓練的模式，檢查模式的設定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at ../data/mygpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config:\n",
      "{\n",
      "  \"_name_or_path\": \"ckiplab/gpt2-base-chinese\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"add_cross_attention\": false,\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": 101,\n",
      "  \"chunk_size_feed_forward\": 0,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"diversity_penalty\": 0.0,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"encoder_no_repeat_ngram_size\": 0,\n",
      "  \"eos_token_id\": 102,\n",
      "  \"finetuning_task\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_beam_groups\": 1,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_scores\": false,\n",
      "  \"pad_token_id\": null,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"return_dict\": true,\n",
      "  \"return_dict_in_generate\": false,\n",
      "  \"sep_token_id\": null,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"temperature\": 1.0,\n",
      "  \"tie_encoder_decoder\": false,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"tokenizer_class\": \"BertTokenizerFast\",\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"transformers_version\": \"4.3.2\",\n",
      "  \"use_bfloat16\": false,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128,\n",
      "  \"xla_device\": null\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "import transformers\n",
    "import tensorflow as tf\n",
    "\n",
    "from transformers import TFGPT2LMHeadModel\n",
    "from transformers import GPT2Tokenizer, BertTokenizer\n",
    "\n",
    "model = TFGPT2LMHeadModel.from_pretrained('../data/mygpt2')\n",
    "tokenizer = BertTokenizer.from_pretrained('../data/tokenizer_bert_base_chinese')\n",
    "\n",
    "num_pieces = 100\n",
    "tokenized_data_path = '../data/tokenized_data/'\n",
    "full_tokenizer=tokenizer\n",
    "output_dir = '../data/gpt2_ft/'\n",
    "\n",
    "model_config = transformers.GPT2Config.from_json_file('../data/mygpt2/config.json')\n",
    "print('config:\\n' + model_config.to_json_string('../data/mygpt2/config.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "882997\n",
      "DatasetSpec(TensorSpec(shape=(), dtype=tf.int32, name=None), TensorShape([]))\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "import numpy as np\n",
    "with open('../data/tokenized_data/tokenized_train_237.txt', 'r') as f:\n",
    "    tokenized_ids = f.readline().strip().split(' ')\n",
    "\n",
    "print(len(tokenized_ids))\n",
    "\n",
    "data = [int(id) for id in tokenized_ids]\n",
    "#print(data[:20])\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(data).window(model_config.n_ctx, drop_remainder=True)\n",
    "print(dataset.element_spec)\n",
    "\n",
    "train_data = []\n",
    "for window in dataset:\n",
    "    train_data.append(np.array([elem.numpy() for elem in window]).astype('int32'))\n",
    "\n",
    "#print(len(train_data))\n",
    "#print(train_data[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(862, 1024)\n",
      "13722\n",
      "100\n",
      "Model: \"tfgp_t2lm_head_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "transformer (TFGPT2MainLayer multiple                  102068736 \n",
      "=================================================================\n",
      "Total params: 102,068,736\n",
      "Trainable params: 102,068,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "StagingError",
     "evalue": "in user code:\n\n    C:\\Users\\tsyo\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\tsyo\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_tf_gpt2.py:700 call  *\n        inputs = input_processing(\n    C:\\Users\\tsyo\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py:374 input_processing  *\n        output[parameter_names[i]] = input\n\n    IndexError: list index out of range\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStagingError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-cc899a3d3de0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# can also use any keras loss fn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    821\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 823\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    824\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m    696\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[1;32m--> 697\u001b[1;33m             *args, **kwds))\n\u001b[0m\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2854\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2855\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2856\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3075\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStagingError\u001b[0m: in user code:\n\n    C:\\Users\\tsyo\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\tsyo\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_tf_gpt2.py:700 call  *\n        inputs = input_processing(\n    C:\\Users\\tsyo\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py:374 input_processing  *\n        output[parameter_names[i]] = input\n\n    IndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "#train_data = np.array(train_data)\n",
    "print(np.array(train_data).shape)\n",
    "#print(train_data[1])\n",
    "print(max(data))\n",
    "print(min(data))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "model.compile(optimizer=optimizer, loss=model.compute_loss) # can also use any keras loss fn\n",
    "model.summary()\n",
    "model.fit(x=train_data, epochs=3, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stride=768\n",
    "batch_size=8\n",
    "n_ctx = model_config.n_ctx\n",
    "\n",
    "print('starting training')\n",
    "now = datetime.now()\n",
    "print('time: {}'.format(now))\n",
    "\n",
    "overall_step = 0\n",
    "running_loss = 0\n",
    "for epoch in range(epochs):\n",
    "    x = np.linspace(0, num_pieces - 1, num_pieces, dtype=np.int32)\n",
    "    random.shuffle(x)\n",
    "    piece_num = 0\n",
    "    for i in x:\n",
    "        with open(tokenized_data_path + 'tokenized_train_{}.txt'.format(i), 'r') as f:\n",
    "            line = f.read().strip()\n",
    "        tokens = line.split()\n",
    "        tokens = [int(token) for token in tokens]\n",
    "        start_point = 0\n",
    "        samples = []\n",
    "        while start_point < len(tokens) - n_ctx:\n",
    "            samples.append(tokens[start_point: start_point + n_ctx])\n",
    "            start_point += stride\n",
    "        if start_point < len(tokens):\n",
    "            samples.append(tokens[len(tokens)-n_ctx:])\n",
    "        random.shuffle(samples)\n",
    "        for step in range(len(samples) // batch_size):  # drop last\n",
    "\n",
    "            #  prepare data\n",
    "            batch = samples[step * batch_size: (step + 1) * batch_size]\n",
    "            batch_inputs = []\n",
    "            for ids in batch:\n",
    "                int_ids = [int(x) for x in ids]\n",
    "                batch_inputs.append(int_ids)\n",
    "            batch_inputs = tf.tensor(batch_inputs).long().to(device)\n",
    "\n",
    "            #  forward pass\n",
    "            outputs = model.forward(input_ids=batch_inputs, labels=batch_inputs)\n",
    "            loss, logits = outputs[:2]\n",
    "\n",
    "            #  get loss\n",
    "            if multi_gpu:\n",
    "                loss = loss.mean()\n",
    "            if gradient_accumulation > 1:\n",
    "                loss = loss / gradient_accumulation\n",
    "\n",
    "            #  loss backward\n",
    "            if fp16:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), max_grad_norm)\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "            #  optimizer step\n",
    "            if (overall_step + 1) % gradient_accumulation == 0:\n",
    "                running_loss += loss.item()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "            if (overall_step + 1) % log_step == 0:\n",
    "                tb_writer.add_scalar('loss', loss.item() * gradient_accumulation, overall_step)\n",
    "                print('now time: {}:{}. Step {} of piece {} of epoch {}, loss {}'.format(\n",
    "                    datetime.now().hour,\n",
    "                    datetime.now().minute,\n",
    "                    step + 1,\n",
    "                    piece_num,\n",
    "                    epoch + 1,\n",
    "                    running_loss * gradient_accumulation / (log_step / gradient_accumulation)))\n",
    "                running_loss = 0\n",
    "            overall_step += 1\n",
    "        piece_num += 1\n",
    "\n",
    "    print('saving model for epoch {}'.format(epoch + 1))\n",
    "    if not os.path.exists(output_dir + 'model_epoch{}'.format(epoch + 1)):\n",
    "        os.mkdir(output_dir + 'model_epoch{}'.format(epoch + 1))\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model\n",
    "    model_to_save.save_pretrained(output_dir + 'model_epoch{}'.format(epoch + 1))\n",
    "    print('epoch {} finished'.format(epoch + 1))\n",
    "\n",
    "\n",
    "print('training finished')\n",
    "then = datetime.now()\n",
    "print('time: {}'.format(then))\n",
    "print('time for training: {}'.format(then - now))\n",
    "\n",
    "\n",
    "if not os.path.exists(output_dir + 'final_model'):\n",
    "    os.mkdir(output_dir + 'final_model')\n",
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "model_to_save.save_pretrained(output_dir + 'final_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 Trainer 類別來調整模型\n",
    "\n",
    "`HuggingFace` 提供了 [`Trainer` class](https://huggingface.co/transformers/main_classes/trainer.html#transformers.Trainer) 作為調整模型參數的工具。在使用 `Trainer` 之前，我們需要先決定模型結構（通常是使用 `model.from_pretrained`），並且據此以 [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments) 來設定訓練用的 hyper-parameters，例如 `learning_rate`, `num_train_epochs`, 和 `per_device_train_batch_size` 等等。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfdcb873f5944ae2a9f9443cb231a647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Generating dataset glue (C:\\Users\\tsyo\\tensorflow_datasets\\glue\\mrpc\\1.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\tsyo\\tensorflow_datasets\\glue\\mrpc\\1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b314bc7af8ba4e3aaea414c800e5e0f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: |          | 0/0 [00:00<?, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1c9c8567607475591a8fc7d50c30c1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: |          | 0/0 [00:00<?, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Downloading https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2Fmrpc_dev_ids.tsv?alt=media&token=ec5c0836-31d5-48f4-b431-7480817f1adc into C:\\Users\\tsyo\\tensorflow_datasets\\downloads\\fire.goog.com_v0_b_mtl-sent-repr.apps.com_o_2FjSIMlCiqs1QSmIykr4IRPnEHjPuGwAz5i40v8K9U0Z8.tsvalt=media&token=ec5c0836-31d5-48f4-b431-7480817f1adc.tmp.9bb6015152474f53a4c8f753c28f0d16...\n",
      "INFO:absl:Downloading https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_test.txt into C:\\Users\\tsyo\\tensorflow_datasets\\downloads\\dl.fbaip.com_sente_sente_msr_parap_test0PdekMcyqYR-w4Rx_d7OTryq0J3RlYRn4rAMajy9Mak.txt.tmp.4f16cd9473ce4561af82f9b1529a6243...\n",
      "INFO:absl:Downloading https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt into C:\\Users\\tsyo\\tensorflow_datasets\\downloads\\dl.fbaip.com_sente_sente_msr_parap_trainfGxPZuQWGBti4Tbd1YNOwQr-OqxPejJ7gcp0Al6mlSk.txt.tmp.5c005523e04f49439765d9ddf80bb88c...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...: |          | 0/? [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling glue-train.tfrecord...:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Done writing glue-train.tfrecord. Number of examples: 3668 (shards: [3668])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation examples...: |          | 0/? [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling glue-validation.tfrecord...:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Done writing glue-validation.tfrecord. Number of examples: 408 (shards: [408])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...: |          | 0/? [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling glue-test.tfrecord...:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Done writing glue-test.tfrecord. Number of examples: 1725 (shards: [1725])\n",
      "INFO:absl:Constructing tf.data.Dataset for split None, from C:\\Users\\tsyo\\tensorflow_datasets\\glue\\mrpc\\1.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset glue downloaded and prepared to C:\\Users\\tsyo\\tensorflow_datasets\\glue\\mrpc\\1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsyo\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\transformers\\data\\processors\\glue.py:67: FutureWarning: This function will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"function\"), FutureWarning)\n",
      "C:\\Users\\tsyo\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\transformers\\data\\processors\\glue.py:175: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, glue_convert_examples_to_features\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "data = tfds.load('glue/mrpc')\n",
    "train_dataset = glue_convert_examples_to_features(data['train'], tokenizer, max_length=128, task='mrpc')\n",
    "train_dataset = train_dataset.shuffle(100).batch(32).repeat(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
