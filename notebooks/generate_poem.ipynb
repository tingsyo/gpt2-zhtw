{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04047f5f",
   "metadata": {},
   "source": [
    "# Generate Poems with Pre-Trained Language Models\n",
    "\n",
    "In this notebook, we are developing a way to generate poem-like content with a pre-trained language model.\n",
    "\n",
    "## Sentence-by-sentence approach\n",
    "\n",
    "It is hard to control the lengthy content generated by the pre-trained model, and hence we design a sentence-by-sentence approach. The basic idea is to generate multiple short sentences at a time, evaluate the meaning-vectors of those short sentences, select the best sentence (so that we may control the flow with the meaning-vectors), and then generate the next sentence with the selected sentence.\n",
    "\n",
    "A rough pseudo-code is like:\n",
    "\n",
    "- starting with the seed_sentence\n",
    "- while(not stopping_criteria):\n",
    "    - generate multiple sentences with `model.generate()`\n",
    "    - post-process the generated sentences:\n",
    "        - evaluate the word-embedding of the seed_sentence, vec_seed\n",
    "        - for s in sentences:\n",
    "            - remove the seed_sentence\n",
    "            - split s into sentences, take the first K, where k=random(1,2,3)\n",
    "            - segment s_k into terms and evaluate the word-embedding, vec_s\n",
    "            - calculate the inner product of (vec_seed, vec_s)\n",
    "        - s_next = argmax(np.dot(vec_seed, vec_s))\n",
    "    - s_next becomes the new seed_sentence\n",
    "    - continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0a57bd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import logging, os, json, argparse\n",
    "from transformers import BertTokenizerFast, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "\n",
    "\n",
    "def decode_generated_ids(generated, tokenizer):\n",
    "    ''' Decode the ids generated by the language model. '''\n",
    "    output=[]\n",
    "    for i in range(10):\n",
    "        text = tokenizer.decode(generated[i], skip_special_tokens= True)    # Decode the generated text\n",
    "        text = text.replace(' ','')                                         # Remove spaces between tokens\n",
    "        text = text.replace(',','，')\n",
    "        output.append(text)\n",
    "    return(output)\n",
    "\n",
    "def generate_new_sentences(input, tokenizer, model):\n",
    "    ''' Generate new sentences with specified model and tokenizer. '''\n",
    "    # Parse seeding string\n",
    "    input_ids = tokenizer.encode(input, return_tensors='pt')\n",
    "    # Generate text\n",
    "    generated = model.generate(input_ids, \n",
    "                            max_length=50,  \n",
    "                            num_return_sequences=10,\n",
    "                            no_repeat_ngram_size=2,\n",
    "                            repetition_penalty=1.05,\n",
    "                            length_penalty=0.8,\n",
    "                            top_p=0.75,\n",
    "                            temperature=.5,\n",
    "                            do_sample=True,\n",
    "                            top_k=128,\n",
    "                            early_stopping=True)\n",
    "    # Decode\n",
    "    output = decode_generated_ids(generated, tokenizer)\n",
    "    # Done\n",
    "    return(output)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0902e958",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('../data/tokenizer_bert_base_chinese', clean_text=True)\n",
    "#model = AutoModelForCausalLM.from_pretrained('../model/ckip', from_tf=True)\n",
    "model = AutoModelForCausalLM.from_pretrained('../model/gpt2pt_gulong', from_tf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac630765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "# Generate random numbers\n",
    "np.random.seed(10)                          # Set random-state\n",
    "total_lines = np.random.randint( 5,15)      # Define the total lines\n",
    "print(total_lines)\n",
    "#\n",
    "seed_sentence = \"巴格達到台北的距離\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebab1500",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:102 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['巴格達到台北的距離有三十公里，而且是在中東地區。美國國務院發言人勃恩斯說，布希總統將於今天稍後訪', '巴格達到台北的距離在二十四小時內，台灣的中華民國是一個主權獨立的國家。他說，中共對台政策的改變，', '巴格達到台北的距離不過，他也說，如果台灣要加入世界衛生組織，必須先經過中國大陸的同意才能入會。他', '巴格達到台北的距離不過，他也強調，中共在台灣海峽附近的軍事演習，對台海兩岸都有利。他說，美國的政', '巴格達到台北的距離但是在台灣的大陸人民，不會有任何理由對他們進行政治迫害。」他說:「我們不希望中', '巴格達到台北的距離但是，這種情況在台灣也不會有任何改變。」他說:「我們必須繼續努力，以便儘快恢復', '巴格達到台北的距離是一個小時。這項協議將使巴國政府在未來幾年內，能夠加速推動經濟發展，並且讓巴拉', '巴格達到台北的距離在十二月二十九日，美國總統柯林頓和中共國家主席江澤民在北京舉行高峰會談，雙方就', '巴格達到台北的距離有二十公里，因此，他們不會因為中共的軍事演習而受到影響。但是，在台灣的外交官也', '巴格達到台北的距離只有一百五十公里，而且在台灣，它的飛行時間比較長。他說:「我們的目標是要把台海']\n"
     ]
    }
   ],
   "source": [
    "generated = generate_new_sentences(seed_sentence, tokenizer, model)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26aed6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "614083\n"
     ]
    }
   ],
   "source": [
    "# Load the word embedding for evaluation\n",
    "import pickle\n",
    "\n",
    "with open('D:\\workspace\\word_embedding\\zh_wiki_word2vec_300.pkl', 'rb') as f:\n",
    "    we = pickle.load(f)\n",
    "\n",
    "print(len(we.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f46542e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vector(word, word_embedding):\n",
    "    ''' Look-up the word vector from the given word-embedding. '''\n",
    "    try:\n",
    "        vec = word_embedding[word]\n",
    "    except KeyError as e:\n",
    "        print(e)\n",
    "        vec = np.zeros((300))    # Return zero vector if not found\n",
    "    return(vec)\n",
    "\n",
    "\n",
    "def evaluate_tokens(tokens, word_embedding):\n",
    "    ''' Evaluate the  '''\n",
    "    n_token = len(tokens)\n",
    "    vec = np.zeros((300))\n",
    "    for i in range(n_token):\n",
    "        vec = vec + get_word_vector(tokens[i], word_embedding)\n",
    "    vec = vec/n_token\n",
    "    return(vec)\n",
    "\n",
    "\n",
    "def postprocess_generated_sentences(sentences, seed_sentence, vdict):\n",
    "    ''' Post-process the generated paragraph. '''\n",
    "    # Define sentence-break symbols\n",
    "    bs = ['，','。','；','！','？']\n",
    "    # Loop through all generated snetences\n",
    "    svecs = []\n",
    "    stokens = []\n",
    "    for s in sentences:\n",
    "        temp = s.replace(seed_sentence, '')     # Remove the seed sentence\n",
    "        tokens = list(jieba.cut(temp))          # Tokenize the sentence with jieba\n",
    "        # Looking for sentence-break symbols\n",
    "        idxs = [i for i, x in enumerate(tokens) if x in bs]\n",
    "        print(len(idxs))\n",
    "        if len(idxs)>1:                         # Keep tokens before the fisrt break\n",
    "            tokens = tokens[idxs[0]+1:idxs[1]]\n",
    "        elif len(idxs)>0:\n",
    "            tokens = tokens[:idxs[0]]\n",
    "        else:\n",
    "            tokens = tokens\n",
    "        svec = evaluate_tokens(tokens, vdict)   # Calculate the word-embedding vectors of the tokens\n",
    "        svecs.append(svec)\n",
    "        stokens.append(tokens)\n",
    "    #\n",
    "    return({'sentences':stokens,'wvectors':svecs})\n",
    "\n",
    "\n",
    "def select_next_sentence(candidates, seed_vec):\n",
    "    ''' Select the best candidate. '''\n",
    "    scores = []\n",
    "    for i in range(len(candidates['sentences'])):\n",
    "        print(candidates['sentences'][i])\n",
    "        score = np.dot(seed_vec, candidates['wvectors'][i])\n",
    "        print(score)\n",
    "        scores.append(score)\n",
    "    return({'sentence':candidates['sentences'][scores.index(max(scores))], \n",
    "            'vector':candidates['wvectors'][scores.index(max(scores))]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7def585",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\tsyo\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.627 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'一百五十公里'\n"
     ]
    }
   ],
   "source": [
    "pps = postprocess_generated_sentences(generated, seed_sentence, we)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e09119e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "巴格達到台北的距離\n",
      "['有', '三十公里']\n",
      "2.065612835053948\n",
      "['在', '二十四小', '時內']\n",
      "2.0885943965330505\n",
      "['不過']\n",
      "2.240431904950311\n",
      "['不過']\n",
      "2.240431904950311\n",
      "['但是', '在', '台灣', '的', '大陸', '人民']\n",
      "1.9036225756816176\n",
      "['但是']\n",
      "1.6261310035297365\n",
      "['是', '一個', '小時']\n",
      "2.4542005939882654\n",
      "['在', '十二月', '二十九日']\n",
      "1.3226899392892093\n",
      "['有', '二十公里']\n",
      "1.8746241015857688\n",
      "['只有', '一百五十公里']\n",
      "0.9610658728828538\n"
     ]
    }
   ],
   "source": [
    "seed_vec = evaluate_tokens(list(jieba.cut(seed_sentence)), we)\n",
    "print(seed_sentence)\n",
    "\n",
    "scores = []\n",
    "for i in range(len(generated)):\n",
    "    print(pps['sentences'][i])\n",
    "    score = np.dot(seed_vec, pps['wvectors'][i])\n",
    "    print(score)\n",
    "    scores.append(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cee9194c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['是', '一個', '小時']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pps['sentences'][scores.index(max(scores))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "81bf933c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:102 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To generate 11 sentences in total.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:102 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "['他們', '的', '目的地']\n",
      "1.9705591310022152\n",
      "['就', '在', '那裡']\n",
      "1.7935014486517125\n",
      "['他們', '一定', '要', '等到', '明年', '才能', '看到', '他']\n",
      "1.4773623678492964\n",
      "['就是', '要', '讓', '這個', '世界', '上', '有', '一個', '人', '能夠', '看見', '他', '的', '蹤', '影']\n",
      "1.8235508858579936\n",
      "['他們', '的', '行動', '已經', '是', '非常', '危險', '的']\n",
      "1.9119363037679267\n",
      "['只不過', '是', '一種', '非常', '危險', '的', '行為']\n",
      "1.7566897225904405\n",
      "['就是', '要', '讓', '別人', '瞭解', '他', '的', '生死']\n",
      "1.5969375718779921\n",
      "['他們', '都', '很', '遙遠']\n",
      "1.9634429421973043\n",
      "['他們', '都', '不會', '像', '楚留香', '那樣']\n",
      "1.5553901897471025\n",
      "['這個', '世界', '上', '有', '很多', '事', '都', '是', '這樣子', '的']\n",
      "1.6968698230985952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:102 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "'?'\n",
      "'」'\n",
      "1\n",
      "1\n",
      "1\n",
      "['在', '那裡']\n",
      "2.5753824364244142\n",
      "['在', '那裡']\n",
      "2.5753824364244142\n",
      "['在', '那裡']\n",
      "2.5753824364244142\n",
      "['在', '那裡']\n",
      "2.5753824364244142\n",
      "['在', '於', '阻止', '他', '的', '人']\n",
      "2.265638248031104\n",
      "['在', '那裡']\n",
      "2.5753824364244142\n",
      "['在', '哪裡', '?', '」']\n",
      "1.1062530919681064\n",
      "['在', '那裡']\n",
      "2.5753824364244142\n",
      "['在', '那裡']\n",
      "2.5753824364244142\n",
      "['在', '那裡']\n",
      "2.5753824364244142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsyo\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in true_divide\n",
      "Setting `pad_token_id` to `eos_token_id`:102 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "'」'\n",
      "1\n",
      "1\n",
      "1\n",
      "[]\n",
      "nan\n",
      "[]\n",
      "nan\n",
      "[]\n",
      "nan\n",
      "['就是', '他們', '的', '朋友']\n",
      "2.690904658188446\n",
      "[]\n",
      "nan\n",
      "['他們', '都', '不', '知道']\n",
      "2.70154428853963\n",
      "['」']\n",
      "0.0\n",
      "[]\n",
      "nan\n",
      "['他們', '的', '人', '都', '已', '經死', '了']\n",
      "2.6765193003630356\n",
      "[]\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:102 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "'「'\n",
      "2\n",
      "1\n",
      "'「'\n",
      "2\n",
      "3\n",
      "3\n",
      "2\n",
      "'」'\n",
      "'：'\n",
      "'「'\n",
      "2\n",
      "0\n",
      "'「'\n",
      "'?'\n",
      "'」'\n",
      "1\n",
      "'「'\n",
      "['「', '這個', '人', '是不是', '因為', '他們', '的', '身份', '而', '改變', '了']\n",
      "nan\n",
      "['就是', '我們', '這個', '男人']\n",
      "nan\n",
      "['「', '我們', '是不是', '也', '不', '知道']\n",
      "nan\n",
      "['不是', '為', '了', '我', '而', '做']\n",
      "nan\n",
      "['就算', '他', '是', '個', '死', '人', '也', '不會', '相應']\n",
      "nan\n",
      "['是', '為', '了', '要', '讓', '人', '知道', '他們', '的', '行', '蹤']\n",
      "nan\n",
      "['」', '他', '說', '：', '「', '我', '知道', '這一點']\n",
      "nan\n",
      "['你們', '這次', '行動', '的', '目的', '是', '要', '讓', '人', '知道', '他們', '是', '在', '想要', '他', '的', '計劃', '中', '遭到', '的']\n",
      "nan\n",
      "['「', '你們', '不是', '要', '看見', '這個', '人', '?', '」']\n",
      "nan\n",
      "['「', '這個', '世界', '上', '有', '很多', '事', '都', '是', '不', '可能', '的', '事']\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:102 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "'」'\n",
      "1\n",
      "[]\n",
      "nan\n",
      "[]\n",
      "nan\n",
      "[]\n",
      "nan\n",
      "[]\n",
      "nan\n",
      "[]\n",
      "nan\n",
      "[]\n",
      "nan\n",
      "[]\n",
      "nan\n",
      "[]\n",
      "nan\n",
      "['」']\n",
      "0.0\n",
      "[]\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:102 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n",
      "'「'\n",
      "2\n",
      "1\n",
      "'「'\n",
      "0\n",
      "'「'\n",
      "'?'\n",
      "'」'\n",
      "1\n",
      "'「'\n",
      "1\n",
      "'「'\n",
      "1\n",
      "'「'\n",
      "1\n",
      "'「'\n",
      "0\n",
      "'「'\n",
      "'?'\n",
      "'」'\n",
      "['而是', '因為', '他', '是', '個', '非常', '聰明', '的', '女人']\n",
      "nan\n",
      "['「', '你們', '都', '是', '這樣子', '的']\n",
      "nan\n",
      "['而是', '想要', '他', '殺', '死', '他']\n",
      "nan\n",
      "['「', '我們', '的', '確有', '一點', '意思']\n",
      "nan\n",
      "['「', '這個', '世界', '上', '有', '一些', '人', '是不是', '能夠', '看', '得', '見', '的', '?', '」']\n",
      "nan\n",
      "['「', '這個', '世界', '上', '有', '很多', '事', '都', '是', '不能', '算是', '事實', '的']\n",
      "nan\n",
      "['「', '我', '知道', '這個', '世界', '上', '有', '很多', '事', '都', '是', '這樣子', '的']\n",
      "nan\n",
      "['「', '你', '知道']\n",
      "nan\n",
      "['「', '我', '不', '知道']\n",
      "nan\n",
      "['「', '我們', '是不是', '要', '殺', '他們', '的', '人', '?', '」']\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:102 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "'」'\n",
      "2\n",
      "0\n",
      "'」'\n",
      "0\n",
      "'」'\n",
      "0\n",
      "'」'\n",
      "2\n",
      "'聰麗'\n",
      "0\n",
      "'：'\n",
      "'「'\n",
      "'?'\n",
      "'」'\n",
      "2\n",
      "2\n",
      "1\n",
      "['」']\n",
      "0.0\n",
      "['而且', '很快', '就會', '變得', '非凡']\n",
      "2.174045268942904\n",
      "['」']\n",
      "0.0\n",
      "['」']\n",
      "0.0\n",
      "['」']\n",
      "0.0\n",
      "['就', '好像', '是', '一個', '很', '聰麗', '的', '人']\n",
      "2.4377726911408506\n",
      "['所以', '她', '才', '會問', '：', '「', '你', '說', '的', '是', '什麼', '?', '」']\n",
      "1.8519584559064883\n",
      "['就', '像是', '一個', '不能', '讓', '別人', '看到', '的', '小鬼', '一樣']\n",
      "2.5564123898508813\n",
      "['忽然', '間', '就', '已', '經變', '得', '很', '奇怪']\n",
      "2.15733491654467\n",
      "['所以', '她', '才', '會', '覺得', '自己', '一定', '會', '想到', '自我']\n",
      "2.536676634685342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:102 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "3\n",
      "0\n",
      "'」'\n",
      "3\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "'每一處'\n",
      "1\n",
      "['就', '好像', '一', '隻', '眼']\n",
      "2.320698607242605\n",
      "['一定', '要', '有', '這種', '感覺']\n",
      "2.7739131254219345\n",
      "['他們', '的', '行動']\n",
      "2.474049664988776\n",
      "['」']\n",
      "0.0\n",
      "['有', '一些', '人', '是不是', '要', '看', '見', '的']\n",
      "2.4674524629943737\n",
      "['每一條', '路', '都', '有', '一片', '狼來', '的', '痕跡']\n",
      "1.9661304287162713\n",
      "['他們', '也', '許還', '是', '會', '看', '得', '見', '的']\n",
      "2.4472775472269057\n",
      "['一定', '要', '有', '一種', '非常', '嚴肅', '的', '態度']\n",
      "2.5787584488465294\n",
      "['每一處', '都', '有', '一些', '小小的', '屍體']\n",
      "1.923661172433699\n",
      "[]\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:102 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "'」'\n",
      "0\n",
      "'」'\n",
      "1\n",
      "0\n",
      "'」'\n",
      "1\n",
      "0\n",
      "'」'\n",
      "1\n",
      "1\n",
      "0\n",
      "'」'\n",
      "0\n",
      "'」'\n",
      "['」']\n",
      "0.0\n",
      "['」']\n",
      "0.0\n",
      "['因為', '他們', '的', '感情', '都', '好像', '是', '一樣']\n",
      "2.8725921333903526\n",
      "['」']\n",
      "0.0\n",
      "['才能', '讓', '人', '瞭解']\n",
      "2.691455452280936\n",
      "['」']\n",
      "0.0\n",
      "['才能', '讓', '人', '看得出', '來']\n",
      "2.6557389053110065\n",
      "['才能', '讓', '他們', '知道']\n",
      "3.034763665752032\n",
      "['」']\n",
      "0.0\n",
      "['」']\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:102 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[]\n",
      "nan\n",
      "[]\n",
      "nan\n",
      "[]\n",
      "nan\n",
      "[]\n",
      "nan\n",
      "[]\n",
      "nan\n",
      "[]\n",
      "nan\n",
      "[]\n",
      "nan\n",
      "[]\n",
      "nan\n",
      "[]\n",
      "nan\n",
      "[]\n",
      "nan\n",
      "1\n",
      "'「'\n",
      "2\n",
      "1\n",
      "'「'\n",
      "1\n",
      "'「'\n",
      "1\n",
      "'「'\n",
      "2\n",
      "0\n",
      "'「'\n",
      "'?'\n",
      "'」'\n",
      "0\n",
      "'「'\n",
      "'?'\n",
      "'」'\n",
      "0\n",
      "'「'\n",
      "'?'\n",
      "'」'\n",
      "2\n",
      "['「', '我', '不是', '這樣子', '的']\n",
      "nan\n",
      "['就是', '要', '讓', '你', '們', '的', '人', '都', '瞭解', '他們']\n",
      "nan\n",
      "['「', '你們', '不能不', '承認', '的']\n",
      "nan\n",
      "['「', '這個', '世界', '上', '有', '很多', '事', '都', '是不是', '事實', '的']\n",
      "nan\n",
      "['「', '我', '不是', '這樣子', '的']\n",
      "nan\n",
      "['而是', '你', '的', '兄弟']\n",
      "nan\n",
      "['「', '你', '是不是', '要', '把', '你', '的', '頭顱', '割', '下來', '?', '」']\n",
      "nan\n",
      "['「', '你', '是不是', '這樣子', '的', '?', '」']\n",
      "nan\n",
      "['「', '這個', '人', '是', '誰', '?', '」']\n",
      "nan\n",
      "['有', '很多', '事', '都', '是', '不能', '回答', '的']\n",
      "nan\n",
      "巴格達到台北的距離\n",
      "他們的目的地\n",
      "在那裡\n",
      "\n",
      "「這個人是不是因為他們的身份而改變了\n",
      "\n",
      "而是因為他是個非常聰明的女人\n",
      "就像是一個不能讓別人看到的小鬼一樣\n",
      "一定要有這種感覺\n",
      "才能讓他們知道\n",
      "\n",
      "「我不是這樣子的\n"
     ]
    }
   ],
   "source": [
    "# Generate random numbers\n",
    "np.random.seed()                 # Set random-state\n",
    "total_lines = np.random.randint( 8,12)      # Define the total lines\n",
    "print('To generate '+str(total_lines)+' sentences in total.')\n",
    "# Generate starting sentence\n",
    "output = []\n",
    "seed_sentence = '巴格達到台北的距離'\n",
    "output.append(seed_sentence)\n",
    "seed_vec = evaluate_tokens(list(jieba.cut(seed_sentence)), we)\n",
    "# Generate followed-up sentences\n",
    "for i in range(total_lines):\n",
    "    generated = generate_new_sentences(seed_sentence, tokenizer, model)\n",
    "    candidates = postprocess_generated_sentences(generated, seed_sentence, we)\n",
    "    selected = select_next_sentence(candidates, seed_vec)\n",
    "    output.append(''.join(selected['sentence']))\n",
    "    seed_vec = selected['vector']\n",
    "    seed_sentence = ''.join(selected['sentence'])\n",
    "# done\n",
    "print('\\n'.join(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b732f76d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
