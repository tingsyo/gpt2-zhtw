{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train my own GPT-2 in ZH_TW\n",
    "\n",
    "這份筆記，是作為自己實作 GPT-2，並利用繁體中文語料訓練的記錄。\n",
    "\n",
    "\n",
    "## Generative Pre-trained Transformer (GPT)\n",
    "\n",
    "Generative Pretrained Transformer (GPT) 是 [OpenAI](https://openai.com/blog/better-language-models/) 開發的一個模型，是目前自然語言處理 (Natural Language Processing, NLP) 領域的 SOTA (state-of-the-art) 模型。2020 年所發表的 [GPT-3](https://en.wikipedia.org/wiki/GPT-3)，光是模型本身就包含了 175B 個參數，除了模型自動產生的文章已經與真人所寫的無異之外，在各種 NLP 的作業上也都取得了不錯的成績，甚至還有研究人員用它來解微分方程。\n",
    "\n",
    "由於 GPT-3 的模型太過龐大，也衍生出很多其他的討論，像是[這篇](https://buzzorange.com/techorange/2020/08/25/gpt-3-ai-model/)文章引用的討論，估計雲端運算的成本約1200萬美金，也預估未來這類模型只有有錢的大企業才能玩得起。本文作為實驗記錄，我們也無須砸大錢去實作 GPT-3，因此我們從有[開源的 GPT-2](https://github.com/openai/gpt-2) 開始實作，並且測試在繁體中文文本上的可行性。\n",
    "\n",
    "事實上 GPT-2 發佈以來，已經有一些簡體中文的 porting，像是：\n",
    "- [gpt2-chinese](https://github.com/Morizeyao/GPT2-Chinese)\n",
    "- [gpp2-ml](https://github.com/imcaspar/gpt2-ml)\n",
    "\n",
    "經過初步研究，OpenAI 的 GPT-2 是建立於 TensorFlow 1.4，因此上述的 repo 也是建立在相同的基礎上。而撰寫本文的現在，最新的 TensorFlow 是 2.4 版， TF 在 2.0 之後有重大的改版，所以我們可能需要從頭檢視並修改原始的模型。\n",
    "\n",
    "\n",
    "## Transformer Model\n",
    "\n",
    "Vaswani 等人在 2017 年提出了注意力機制和 transformer model (Vaswani et al., 2017)，改善了用 RNN 處理語言模型的幾個問題，而這個 transformer model 也是 GPT 的基礎。關於 Transformer model 的介紹，可以參考以下兩個資訊：\n",
    "\n",
    "- [The illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) by Jay Alammar\n",
    "- [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html) by Harvard NLP\n",
    "\n",
    "簡單的說，transformer model 是加入了 attention mechanism 的 encoder-decoder 架構，在這個測試裡，我們先直接使用 [Hugging Face](https://huggingface.co/) 提供的 GPT-2 implementation，以及自己準備的語料來訓練。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 參考資料\n",
    "- Arshabhi Kayal, 2020. [Train GPT-2 in your own language](https://towardsdatascience.com/train-gpt-2-in-your-own-language-fc6ad4d60171):A step-by-step guide to train your own GPT-2 model for text generation in your choice of language from scratch\n",
    "\n",
    "- Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017). **Attention Is All You Need**. ArXiv:1706.03762 [Cs]. [http://arxiv.org/abs/1706.03762](http://arxiv.org/abs/1706.03762)\n",
    "\n",
    "- Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). **Language Models are Unsupervised Multitask Learners**. /paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe\n",
    "\n",
    "- Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., … Amodei, D. (2020). **Language Models are Few-Shot Learners**. ArXiv:2005.14165 [Cs]. http://arxiv.org/abs/2005.14165\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中文 GPT2\n",
    "\n",
    "目前中文的 GPT-2 模型大致有幾個：\n",
    "- [gpt2-ml]()\n",
    "- [gpt2-chinese]()\n",
    "- [ckip-transformers](https://github.com/ckiplab/ckip-transformers)\n",
    "\n",
    "而訓練的語料目前還是以簡體居多，可以參考「[为中文自然语言处理领域发展贡献语料](https://github.com/brightmart/nlp_chinese_corpus)」：\n",
    "\n",
    "- **维基百科json版(wiki2019zh)**[Google Drive](https://drive.google.com/file/d/1EdHUZIDpgcBoSqbjlfNKJ3b1t0XIUjbt/view?usp=sharing)\n",
    "    - 104万个词条(1,043,224条; 原始文件大小1.6G，压缩文件519M；数据更新时间：2019.2.7)\n",
    "\n",
    "- **新闻语料json版(news2016zh)**[Google Drive](https://drive.google.com/file/d/1TMKu1FpTr6kcjWXWlQHX7YJsMfhhcVKp/view?usp=sharing)\n",
    "    - 250万篇新闻。新闻来源涵盖了6.3万个媒体，含标题、关键词、描述、正文。( 原始数据9G，压缩文件3.6G；新闻内容跨度：2014-2016年)\n",
    "\n",
    "- **百科类问答json版(baike2018qa)**[Google Drive](https://drive.google.com/open?id=1_vgGQZpfSxN_Ng9iTAvE7hM3Z7NVwXP2)\n",
    "    - 150万个问答，预先过滤过的、高质量问题和答案，每个问题属于一个类别。总共有492个类别，其中频率达到或超过10次的类别有434个。( 原始数据1G多，压缩文件663M；数据更新时间：2018年)\n",
    "\n",
    "- **社区问答json版(webtext2019zh)**[Google Drive](https://drive.google.com/open?id=1u2yW_XohbYL2YAK6Bzc5XrngHstQTf0v)\n",
    "    - 410万个问答( 过滤后数据3.7G，压缩文件1.7G；数据跨度：2015-2016年)\n",
    "    - 含有410万个预先过滤过的、高质量问题和回复。每个问题属于一个【话题】，总共有2.8万个各式话题，话题包罗万象。\n",
    "    - 从1400万个原始问答中，筛选出至少获得3个点赞以上的的答案，代表了回复的内容比较不错或有趣，从而获得高质量的数据集。\n",
    "    - 除了对每个问题对应一个话题、问题的描述、一个或多个回复外，每个回复还带有点赞数、回复ID、回复者的标签。\n",
    "    - 数据集划分：数据去重并分成三个部分。训练集：412万；验证集：6.8万；测试集a：6.8万；测试集b，不提供下载。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 測試 HuggingFace transformers\n",
    "\n",
    "在建立自己的 GPT-2 模型之前，我們先用別人已經訓練好的中文模型來試試看。我們以 HuggingFace + CKIP 的 transformers 來測試，在測試之前需要確認安裝的套件：\n",
    "\n",
    "- transformers\n",
    "- pytorch\n",
    "- gensim\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:102 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] 今 天 ， 我 抱 著 沈 重 的 心 情 [SEP] 讓 我 的 家 庭 更 有 福 氣 。 我 們 必 須 能 夠 完 成 這 個 計 劃 。 」 2010 年 4 月 ， 有 報 道 稱 ， 艾 維 斯 與 妻 子 於 北 蘇 門 答 臘 的 [UNK] 中 國 醫 藥 學 院 附 設 醫 院 [UNK] 出 現 [UNK] 腎 結 石 的 事 件 。 此 事 件 引 發 中 國 官 方 緊 張 ， 美 國 國 務 院 對 此 事 一 直 拒 絕 發 表 評 論 。 此 事 件 引 發 國 際 媒 體 和 華 僑 多 方 報 導 ， 一 些 學 者 認 為 艾 維 斯 是 中 國 醫 學 院 附 設 醫 院 的 創 辦 人 之 一 。 中 國 大 陸 網 絡 媒 體 大 幅 報 導 ， 此 事 件 亦 引 發 外 交 與 公 共 領 域 的 關 注 。 2009 年 7 月 23 日 ， 中 華 人 民 共 和 國 外 交 部 發 言 人 在 對 艾 維 斯 與 其 妻 子 的 死 訊 中 表 示 ， 艾 維 斯 因 與 丈 夫 和 其 丈 夫 的 子 女 同 歸 於 盡 ， 對 該 院 的 前 途 更 感 憂 慮 。 2011 年 4 月 4 日 ， 在 菲 國 首 都 馬 尼 拉 遭 綁 架 的 菲 律 賓 籍 男 子 [UNK] [UNK] 被 綁 架 、 其 妻 子 和 三 個 妻 子 因 不 滿 公 民 權 被 迫 被 羈 押'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "model = AutoModelForCausalLM.from_pretrained('ckiplab/gpt2-base-chinese')\n",
    "\n",
    "seed_str='今天，我抱著沈重的心情'\n",
    "input_ids = tokenizer.encode(seed_str, return_tensors='pt')\n",
    "test1 = model.generate(input_ids, do_sample=True, max_length=300)\n",
    "tokenizer.decode(test1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看起來是可以運作的，接下來我們就進一步訓練自己的 GPT-2模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
