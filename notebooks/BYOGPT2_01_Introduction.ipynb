{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Your Own GPT-2 in ZH_TW\n",
    "\n",
    "這份筆記，是作為自己實作 GPT-2，並利用繁體中文語料訓練的記錄。\n",
    "\n",
    "\n",
    "## 1. Generative Pre-trained Transformer (GPT)\n",
    "\n",
    "Generative Pretrained Transformer (GPT) 是 [OpenAI](https://openai.com/blog/better-language-models/) 開發的一個模型，是目前自然語言處理 (Natural Language Processing, NLP) 領域的 SOTA (state-of-the-art) 模型。2020 年所發表的 [GPT-3](https://en.wikipedia.org/wiki/GPT-3)，光是模型本身就包含了 175B 個參數，除了模型自動產生的文章已經與真人所寫的無異之外，在各種 NLP 的作業上也都取得了不錯的成績，甚至還有研究人員用它來解微分方程。\n",
    "\n",
    "由於 GPT-3 的模型太過龐大，也衍生出很多其他的討論，像是[這篇](https://buzzorange.com/techorange/2020/08/25/gpt-3-ai-model/)文章引用的討論，估計雲端運算的成本約1200萬美金，也預估未來這類模型只有有錢的大企業才能玩得起。本文作為實驗記錄，我們也無須砸大錢去實作 GPT-3，因此我們從有[開源的 GPT-2](https://github.com/openai/gpt-2) 開始實作，並且測試在繁體中文文本上的可行性。\n",
    "\n",
    "事實上 GPT-2 發佈以來，已經有一些簡體中文的 porting，像是：\n",
    "- [gpt2-chinese](https://github.com/Morizeyao/GPT2-Chinese)\n",
    "- [gpp2-ml](https://github.com/imcaspar/gpt2-ml)\n",
    "\n",
    "經過初步研究，OpenAI 的 GPT-2 是建立於 TensorFlow 1.4，因此上述的 repo 也是建立在相同的基礎上。而撰寫本文的現在，最新的 TensorFlow 是 2.4 版， TF 在 2.0 之後有重大的改版，所以我們可能需要從頭檢視並修改原始的模型。\n",
    "\n",
    "\n",
    "## Transformer Model\n",
    "\n",
    "Vaswani 等人在 2017 年提出了注意力機制和 transformer model (Vaswani et al., 2017)，改善了用 RNN 處理語言模型的幾個問題，而這個 transformer model 也是 GPT 的基礎。關於 Transformer model 的介紹，可以參考以下兩個資訊：\n",
    "\n",
    "- [The illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) by Jay Alammar\n",
    "- [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html) by Harvard NLP\n",
    "\n",
    "簡單的說，transformer model 是加入了 attention mechanism 的 encoder-decoder 架構，在這個測試裡，我們先直接使用 [Hugging Face](https://huggingface.co/) 提供的 GPT-2 implementation，以及自己準備的語料來訓練。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 參考資料\n",
    "- Arshabhi Kayal, 2020. [Train GPT-2 in your own language](https://towardsdatascience.com/train-gpt-2-in-your-own-language-fc6ad4d60171):A step-by-step guide to train your own GPT-2 model for text generation in your choice of language from scratch\n",
    "\n",
    "- Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017). **Attention Is All You Need**. ArXiv:1706.03762 [Cs]. [http://arxiv.org/abs/1706.03762](http://arxiv.org/abs/1706.03762)\n",
    "\n",
    "- Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). **Language Models are Unsupervised Multitask Learners**. /paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe\n",
    "\n",
    "- Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., … Amodei, D. (2020). **Language Models are Few-Shot Learners**. ArXiv:2005.14165 [Cs]. http://arxiv.org/abs/2005.14165\n",
    "\n",
    "- [How to generate text: using different decoding methods for language generation with Transformers](https://huggingface.co/blog/how-to-generate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中文 GPT2\n",
    "\n",
    "目前中文的 GPT-2 模型大致有幾個：\n",
    "- [gpt2-ml]()\n",
    "- [gpt2-chinese]()\n",
    "- [ckip-transformers](https://github.com/ckiplab/ckip-transformers)\n",
    "\n",
    "而訓練的語料目前還是以簡體居多，可以參考「[为中文自然语言处理领域发展贡献语料](https://github.com/brightmart/nlp_chinese_corpus)」：\n",
    "\n",
    "- **维基百科json版(wiki2019zh)**[Google Drive](https://drive.google.com/file/d/1EdHUZIDpgcBoSqbjlfNKJ3b1t0XIUjbt/view?usp=sharing)\n",
    "    - 104万个词条(1,043,224条; 原始文件大小1.6G，压缩文件519M；数据更新时间：2019.2.7)\n",
    "\n",
    "- **新闻语料json版(news2016zh)**[Google Drive](https://drive.google.com/file/d/1TMKu1FpTr6kcjWXWlQHX7YJsMfhhcVKp/view?usp=sharing)\n",
    "    - 250万篇新闻。新闻来源涵盖了6.3万个媒体，含标题、关键词、描述、正文。( 原始数据9G，压缩文件3.6G；新闻内容跨度：2014-2016年)\n",
    "\n",
    "- **百科类问答json版(baike2018qa)**[Google Drive](https://drive.google.com/open?id=1_vgGQZpfSxN_Ng9iTAvE7hM3Z7NVwXP2)\n",
    "    - 150万个问答，预先过滤过的、高质量问题和答案，每个问题属于一个类别。总共有492个类别，其中频率达到或超过10次的类别有434个。( 原始数据1G多，压缩文件663M；数据更新时间：2018年)\n",
    "\n",
    "- **社区问答json版(webtext2019zh)**[Google Drive](https://drive.google.com/open?id=1u2yW_XohbYL2YAK6Bzc5XrngHstQTf0v)\n",
    "    - 410万个问答( 过滤后数据3.7G，压缩文件1.7G；数据跨度：2015-2016年)\n",
    "    - 含有410万个预先过滤过的、高质量问题和回复。每个问题属于一个【话题】，总共有2.8万个各式话题，话题包罗万象。\n",
    "    - 从1400万个原始问答中，筛选出至少获得3个点赞以上的的答案，代表了回复的内容比较不错或有趣，从而获得高质量的数据集。\n",
    "    - 除了对每个问题对应一个话题、问题的描述、一个或多个回复外，每个回复还带有点赞数、回复ID、回复者的标签。\n",
    "    - 数据集划分：数据去重并分成三个部分。训练集：412万；验证集：6.8万；测试集a：6.8万；测试集b，不提供下载。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 測試 HuggingFace transformers\n",
    "\n",
    "在建立自己的 GPT-2 模型之前，我們先用別人已經訓練好的中文模型來試試看。我們以 HuggingFace + CKIP 的 transformers 來測試，在測試之前需要確認安裝的套件：\n",
    "\n",
    "- transformers\n",
    "- pytorch\n",
    "- gensim\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:102 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天天氣很好，我想出門去玩，且沒人會用錢買禮物。」他和在場的女子一起被送回了地球的洞穴中心看管家開始閱讀《星際大戰》和其它漫畫書籍並進行文學創作及動畫設計；參加同年10月至11日播放「新世紀福爾摩莎·羅德里格斯影展舉辦的第4屆音樂節演唱會－團體獎得主愛麗絲·洛西諾夫（）首次擔任觀眾席位的成員是約翰\n",
      "今天天氣很好，我想出門去玩，是說你們不知道自己身體狀況。」，後來因為網路上流傳聲音的宣洩而退役）於2008年12月29日晚間10時在華盛頓州阿密約翰霍普金斯大學醫院舉辦個人生涯發表會('s)。2007至2009獲得美國《》雜誌評選「最佳傑作獎」（2011-2014）、2013賽季美式足球名將皮爾·杜威與2018賽末英雄尼克特共同主持創意設計\n",
      "今天天氣很好，我想出門去玩，然也不知道。」有網友反駁說：「那是王自己的夢？他們都在為你做事了！」、高麗世代政治家石野洋平等人亦認同吳國君所言：中華民族應該可以領導下一個統帥體系嗎？何況這才算真正實現?難道沒時間來解釋權力呢？眾多支持者對於公共議題表示憤怒與驚訝和懷疑態度；並要求其辭職前必須\n",
      "今天天氣很好，我想出門去玩，你們這裡有夢。」後來他和小剛看到一個地方說:「自己是中國人！』然而被問起，並且：『沒什麼幸福嗎？能給大家帶回就算了呢?』；另外還記得最近幾年的拍攝計畫書全部都在工作中不會完成所以不曉歡迎。2012/11賽季結束前夕劉德華離開韓劇圈返臺擔任導演兼編舞、監製等職務（目標則為三\n",
      "今天天氣很好，我想出門去玩，然就以為我沒有睡覺。」他說「不知道大概來的真正原因是，現在這麼多人都未能取得認證、但對事情十分關心並非所難（比如）其實該怎樣要學習到什麼？每次看電影時總會發生幾個故意把劇本提前曝光了！雖然一句話可能被引用過來，但最終我只講懂美國語言等級方面問題；甚至還談到政治考慮部\n",
      "   id                                               text\n",
      "0   1  今天天氣很好，我想出門去玩，且沒人會用錢買禮物。」他和在場的女子一起被送回了地球的洞穴中心看...\n",
      "1   2  今天天氣很好，我想出門去玩，是說你們不知道自己身體狀況。」，後來因為網路上流傳聲音的宣洩而退...\n",
      "2   3  今天天氣很好，我想出門去玩，然也不知道。」有網友反駁說：「那是王自己的夢？他們都在為你做事了...\n",
      "3   4  今天天氣很好，我想出門去玩，你們這裡有夢。」後來他和小剛看到一個地方說:「自己是中國人！』然...\n",
      "4   5  今天天氣很好，我想出門去玩，然就以為我沒有睡覺。」他說「不知道大概來的真正原因是，現在這麼多...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "model = AutoModelForCausalLM.from_pretrained('ckiplab/gpt2-base-chinese')\n",
    "\n",
    "seed_str='今天天氣很好，我想出門去玩，'\n",
    "input_ids = tokenizer.encode(seed_str, return_tensors='pt')\n",
    "\n",
    "\n",
    "generated = model.generate(input_ids, \n",
    "                            max_length=150,  \n",
    "                            num_return_sequences=5,\n",
    "                            no_repeat_ngram_size=2,\n",
    "                            repetition_penalty=1.5,\n",
    "                            top_p=0.92,\n",
    "                            temperature=.85,\n",
    "                            do_sample=True,\n",
    "                            top_k=125,\n",
    "                            early_stopping=True)\n",
    "output=[]\n",
    "for i in range(5):\n",
    "    text = tokenizer.decode(generated[i], \n",
    "            skip_special_tokens= True)          # Decode the generated text\n",
    "    text = text.replace(' ','')                 # Remove spaces between tokens\n",
    "    trial = {'id':i+1, 'text': text}\n",
    "    print(text)\n",
    "    output.append(trial)\n",
    "\n",
    "import pandas as pd\n",
    "print(pd.DataFrame(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看起來是可以運作的，接下來我們就進一步訓練自己的 GPT-2模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(output).to_csv('../workspace/byogpt2_01_generated_text.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1,今天天氣很好，我想出門去玩，你就跟你一起去玩。你說的就是你的事，他們有點感情，把自己的事情都說在來說話，我希望能用自己的話來好好把事情講成話來講話，讓你們的朋友很快樂。」另外，李克勤的家人也會到場，並向記者們說明李克勤的事情。2014年12月8日，上海市人民政府和上海航運集團簽訂一份合同。李克勤在《上海證券報》上發表有關上海證券市場發展的有關意見。2017年6月31日，李克勤在上海大學生運動演講指出：「儘管上海證券市場存在著一些問題不能解決，但在世界上大多數國家都會有一個自主發展的社會，當時世界上很少有一個自己生產的國家有一個自己的企業，因為它不會說出這個地方，而不是說出這個國家生產的。」李克勤因為一些小\n",
    "\n",
    "2,今天天氣很好，我想出門去玩，覺不好玩。」2014年9月5日晚在中公佈《》，《天龍》第三季的原型音源配音。10月23日在正式發表第四季的第六季，將會更新《》配音。11月23日在上公佈第二季的音源配音。11月30日起，主演了《》。第二季中除了第四季的原型音源配音外，也開始播放預告；11月10日，第二季的聲優擔任主要角色及製作人，並於2017年10月14日於音源節目《東京國家》上首次亮相；同年12月8日，《》開始播放預告，2019年1月11日起《》結束。2016年8月22日，隨著《》開播，的音源開始在以及官方的頻道分別播放預告片和綜合版。首個個人演唱會為《》，2017年9月26日起於網站發佈\n",
    "\n",
    "3,今天天氣很好，我想出門去玩，不過會盡量和我們一起出門。」最後他打響個人表演獲得人氣獎。2015年3月18日，由楊詠春、張凱貞以及其他演藝人共同主辦的「天一夜情」演唱會上，中國大陸方面決定參加大陸方面的表演，其餘大陸演員包括陳嘉玲、林子佼、沈君威、高曉晴、黃霑等演員。此演唱會為了推廣國語流行音樂，也是陳嘉玲、高曉晴和張凱貞等演員出道六年後，首次以小型歌曲參加演出；而張凱貞在臺灣方面曾主動邀請她到大陸參加演出。2016年8月時間，陳嘉玲和前新浪娛樂公司總經理葉明偉共同參加在廣州開始的「天一夜情」演唱會，其餘的演員參與演出。2017年6月30日，「天一夜情」演唱會上，陳嘉玲以及高曉晴的以及臺灣藝人許芷蕾也獲邀進唱，\n",
    "\n",
    "4,今天天氣很好，我想出門去玩，你一向沒有你一樣的，我最喜歡你的在美國，因為這張一首最近幾次被的用於表演。2007年12月3日，華納兄弟在臺灣上市。根據華納兄弟製作公司的一項官方報告，是一款平臺於1997年4月在日本發行。在2004年2月17日，於西歐、北大西洋、大洋洲和非洲發行，這次是第一款以為主的平臺的平臺遊戲。包含了許多平臺，如，，、和，並且包含了許多遊戲配件。還包含了。包含了，（）和，。在2006年6月發售的平臺上，也包含了，，，。在2012年8月，被美國電子遊戲公司所發售，並且在它的平臺上也擁有。在2013年6月，被、、和等多個國\n",
    "\n",
    "5,今天天氣很好，我想出門去玩，是當然。」不可思議的有：「有那名人才說出來了！」張惠妹（陳寶春），又名「周子」；張惠妹出身於江南，由她的學經歷轉職以前主張做國學，或因為父親有過失，在外地受過教育，但未來到香港，不被政府當做國學。在大學就讀，張惠妹自稱張氏的男兒。因為兒子成祖，其父母在大陸還沒有被外界認同的經歷。有些人對她感到失望，有些人甚至認為她可以為學校寫好文章。有的人把張惠妹稱為張惠妹。她們稱她為周子。張惠妹是廣東人，在當地有不少國外歌手有粵語歌唱，張惠妹的歌是粵語，有些歌是粵語歌。因為周子在海外是第一個有國外歌唱活動的國家。從前都有香港人來粵音樂學校打工，在成祖大陸開辦音樂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
