{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中文語料的 Tokenization for Transformer Models\n",
    "\n",
    "原本參考的[Train GPT-2 in your own language](https://towardsdatascience.com/train-gpt-2-in-your-own-language-fc6ad4d60171)中，tokenization的部份對中文並不適用，我們進一步參考了其他文章（[[1]](https://clay-atlas.com/blog/2020/06/30/pytorch-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-hugging-face-%E6%89%80%E6%8F%90%E4%BE%9B%E7%9A%84-transformers-%E4%BB%A5-bert-%E7%82%BA%E4%BE%8B/)，[[2]](https://zhuanlan.zhihu.com/p/120315111)，[[3]](https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html)，[[4]](https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a)），來進行我們的 tokenization 作業。\n",
    "\n",
    "在 [Working with Hugging Face Transformers and TF 2.0](https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a) 中提到，transformer model 實際運作的流程基本上都依循：\n",
    "\n",
    "> Tokenizer definition → Tokenization of Documents → Model Definition → Model Training →Inference\n",
    "\n",
    "因此，下面我們就從中文 tokenizer 的定義開始。\n",
    "\n",
    "### Reference\n",
    "1.[如何使用 hugging face 所提供的 transformers 以 bert / PyTorch 為例](https://clay-atlas.com/blog/2020/06/30/pytorch-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-hugging-face-%E6%89%80%E6%8F%90%E4%BE%9B%E7%9A%84-transformers-%E4%BB%A5-bert-%E7%82%BA%E4%BE%8B/)\n",
    "\n",
    "2.[Huggingface简介及BERT代码浅析](https://zhuanlan.zhihu.com/p/120315111)\n",
    "\n",
    "3.[進擊的 BERT：NLP 界的巨人之力與遷移學習](https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html)\n",
    "\n",
    "4.[Working with Hugging Face Transformers and TF 2.0](https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a)\n",
    "\n",
    "5.[中文GPT2预训练实战](https://finisky.github.io/2020/05/01/pretrainchinesegpt/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 淺談語言的標記化（tokenization）\n",
    "\n",
    "標記化（[tokenization](https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization)）屬於詞法分析（[lexical analysis](https://en.wikipedia.org/wiki/Lexical_analysis)）的一個部分，即將輸入字符串分割為標記、進而將標記進行分類的過程。生成的標記隨後便被用來進行語法分析。依據分析的目的不同，標記化可以有很多不同的作法，例如「斷詞」（[word segmentation](https://en.wikipedia.org/wiki/Text_segmentation#Word_segmentation)）就是把每個詞彙轉換成一個標記（token）。\n",
    "\n",
    "以這句英文為例，`The quick brown fox jumps over the lazy dog` 的 word-based-tokenization in XML format 會成為：\n",
    "```\n",
    "<sentence>\n",
    "  <word>The</word>\n",
    "  <word>quick</word>\n",
    "  <word>brown</word>\n",
    "  <word>fox</word>\n",
    "  <word>jumps</word>\n",
    "  <word>over</word>\n",
    "  <word>the</word>\n",
    "  <word>lazy</word>\n",
    "  <word>dog</word>\n",
    "</sentence>\n",
    "```\n",
    "\n",
    "當然斷詞並不是標記化的唯一方法，我們可以把每個字作為獨立的標記（character-based-tokenization），或是把每個句子當做一個標記（sentence-based-tokenization），甚至可以把每個位元當做獨立的標記（byte-based-tokenization）。\n",
    "\n",
    "Hugging Face 的 [`transformers`](https://huggingface.co/transformers/index.html) 套件本身就提供了眾多的標記化工具（[Tokenizer](https://huggingface.co/transformers/main_classes/tokenizer.html)）可供使用。[GPT2](https://huggingface.co/transformers/model_doc/gpt2.html)模型本身有專屬對應的的 [GPT2Tokenizer](https://huggingface.co/transformers/model_doc/gpt2.html#gpt2tokenizer)，屬於 [BPE tokenizer (Byte-Pair-Encoding)](https://medium.com/@pierre_guillou/byte-level-bpe-an-universal-tokenizer-but-aff932332ffe)，理論上 [BPE tokenizer](https://medium.com/@pierre_guillou/byte-level-bpe-an-universal-tokenizer-but-aff932332ffe)是不受限於語言的，但是對於中日韓文這種 multi-byte-character 的語言來說很容易因為「例外字元」出問題，因此還是 word-level- 或是 character-level-tokenization 比較合適。\n",
    "\n",
    "[Transformers](https://huggingface.co/transformers/index.html) 套件並沒有中文斷詞的功能，而網路上可以看到的中文 GPT2 範例，都是用 Google 釋出的的 [BertTokenizer](https://huggingface.co/transformers/model_doc/bert.html#berttokenizer)，接下來我們就用[transformers](https://huggingface.co/transformers/index.html) 套件內建的工具做一些簡單的測試："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent: 今天天氣真 Good。\n",
      "sent_token: ['[CLS]', '今', '天', '天', '氣', '真', '[UNK]', '。', '[SEP]']\n",
      "encode: [101, 791, 1921, 1921, 3706, 4696, 100, 511, 102]\n",
      "decode: ['[CLS]', '今', '天', '天', '氣', '真', '[UNK]', '。', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Tokenizer and Bert Model\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-chinese')\n",
    "#embedding = AutoModel.from_pretrained('bert-base-chinese')\n",
    "\n",
    "\n",
    "# Preprocess\n",
    "sent = '今天天氣真 Good。'\n",
    "sent_token = ['[CLS]'] + tokenizer.tokenize(sent) + ['[SEP]']\n",
    "sent_token_encode = tokenizer.convert_tokens_to_ids(sent_token)\n",
    "sent_token_decode = tokenizer.convert_ids_to_tokens(sent_token_encode)\n",
    "\n",
    "print('sent:', sent)\n",
    "print('sent_token:', sent_token)\n",
    "print('encode:', sent_token_encode)\n",
    "print('decode:', sent_token_decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 測試稍大的語料庫\n",
    "\n",
    "我們剛才針對單一句子的測試成功，接下來要測試稍大的語料庫，我們以 500篇 wikipedia上長度超過500字的中文文章為例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents to encode: 500\n",
      "Sentences encoded: 672846\n"
     ]
    }
   ],
   "source": [
    "corpus_dir = '../data/test_wiki500/'\n",
    "model_path = '../data/tokenizer_bert_base_chinese'\n",
    "\n",
    "\n",
    "def encode_corpus(corpus_path):\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    from transformers import BertTokenizerFast\n",
    "    # Tokenizer and Bert Model\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-chinese\")\n",
    "    #tokenizer.save_pretrained(model_path)\n",
    "    #\n",
    "    paths = [str(x) for x in Path(corpus_path).glob(\"**/*.txt\")]\n",
    "    print('Documents to encode: '+str(len(paths)))\n",
    "    data = []\n",
    "    for furi in paths:\n",
    "        with open(furi,'r', encoding=\"utf8\") as f:\n",
    "            text = f.readlines()\n",
    "            for sent in text[0].split(' '):\n",
    "                sent_token = ['[CLS]'] + tokenizer.tokenize(sent) + ['[SEP]']\n",
    "                sent_token_encode = tokenizer.convert_tokens_to_ids(sent_token)\n",
    "                data.append(sent_token_encode)\n",
    "    print('Sentences encoded: '+str(len(data)))\n",
    "    return(data)\n",
    "\n",
    "data = encode_corpus(corpus_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encode: [101, 2119, 1558, 102]\n",
      "decode: ['[CLS]', '學', '問', '[SEP]']\n",
      "encode: [101, 3229, 7279, 4638, 7269, 4764, 5023, 2853, 6496, 4638, 3149, 7030, 7302, 913, 102]\n",
      "decode: ['[CLS]', '時', '間', '的', '長', '短', '等', '抽', '象', '的', '數', '量', '關', '係', '[SEP]']\n",
      "encode: [101, 3683, 1963, 3229, 7279, 1606, 855, 3300, 3189, 102]\n",
      "decode: ['[CLS]', '比', '如', '時', '間', '單', '位', '有', '日', '[SEP]']\n",
      "encode: [101, 2108, 5059, 1469, 2399, 5023, 102]\n",
      "decode: ['[CLS]', '季', '節', '和', '年', '等', '[SEP]']\n",
      "encode: [101, 5050, 6123, 102]\n",
      "decode: ['[CLS]', '算', '術', '[SEP]']\n",
      "encode: [101, 1217, 3938, 733, 7370, 102]\n",
      "decode: ['[CLS]', '加', '減', '乘', '除', '[SEP]']\n",
      "encode: [101, 738, 5632, 4197, 5445, 4197, 1765, 4496, 4495, 749, 102]\n",
      "decode: ['[CLS]', '也', '自', '然', '而', '然', '地', '產', '生', '了', '[SEP]']\n",
      "encode: [101, 3644, 1380, 677, 3295, 3300, 6882, 6258, 1914, 679, 1398, 4638, 6250, 3149, 5143, 5186, 102]\n",
      "decode: ['[CLS]', '歷', '史', '上', '曾', '有', '過', '許', '多', '不', '同', '的', '記', '數', '系', '統', '[SEP]']\n",
      "encode: [101, 1762, 3297, 1159, 3300, 3644, 1380, 6250, 7087, 4638, 3229, 952, 102]\n",
      "decode: ['[CLS]', '在', '最', '初', '有', '歷', '史', '記', '錄', '的', '時', '候', '[SEP]']\n",
      "encode: [101, 4158, 4747, 6237, 3149, 2099, 7279, 4638, 7302, 913, 102]\n",
      "decode: ['[CLS]', '為', '瞭', '解', '數', '字', '間', '的', '關', '係', '[SEP]']\n",
      "encode: [101, 4158, 749, 3947, 7030, 1759, 1765, 102]\n",
      "decode: ['[CLS]', '為', '了', '測', '量', '土', '地', '[SEP]']\n",
      "encode: [101, 809, 1350, 4158, 749, 7521, 3947, 1921, 3152, 752, 816, 5445, 2501, 2768, 4638, 102]\n",
      "decode: ['[CLS]', '以', '及', '為', '了', '預', '測', '天', '文', '事', '件', '而', '形', '成', '的', '[SEP]']\n",
      "encode: [101, 5178, 3539, 102]\n",
      "decode: ['[CLS]', '結', '構', '[SEP]']\n",
      "encode: [101, 4958, 7279, 1350, 3229, 7279, 3175, 7481, 4638, 4777, 4955, 102]\n",
      "decode: ['[CLS]', '空', '間', '及', '時', '間', '方', '面', '的', '研', '究', '[SEP]']\n",
      "encode: [101, 1168, 749, 102]\n",
      "decode: ['[CLS]', '到', '了', '[SEP]']\n",
      "encode: [101, 686, 5145, 102]\n",
      "decode: ['[CLS]', '世', '紀', '[SEP]']\n",
      "encode: [101, 5050, 6123, 102]\n",
      "decode: ['[CLS]', '算', '術', '[SEP]']\n",
      "encode: [101, 2544, 4948, 1146, 4638, 3519, 2573, 738, 1762, 3634, 3229, 2501, 2768, 102]\n",
      "decode: ['[CLS]', '微', '積', '分', '的', '概', '念', '也', '在', '此', '時', '形', '成', '[SEP]']\n",
      "encode: [101, 7401, 5865, 3149, 2119, 6752, 1403, 2501, 2466, 1265, 102]\n",
      "decode: ['[CLS]', '隨', '著', '數', '學', '轉', '向', '形', '式', '化', '[SEP]']\n",
      "encode: [101, 2537, 1367, 5635, 791, 102]\n",
      "decode: ['[CLS]', '從', '古', '至', '今', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "for row in data[100:120]:\n",
    "    sent_token_encode = row\n",
    "    sent_token_decode = tokenizer.convert_ids_to_tokens(sent_token_encode)\n",
    "    print('encode:', sent_token_encode)\n",
    "    print('decode:', sent_token_decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../data/encoded_wiki500.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
