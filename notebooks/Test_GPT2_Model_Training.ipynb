{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a8c903b",
   "metadata": {},
   "source": [
    "在上一篇筆記裡我們測試了 Model.Config 對實際儲存大小的影響，接下來我們要進一步研究模型訓練的方式。這裡的主要參考資料是 hugginface 提供的 [run_clm.py](https://github.com/huggingface/transformers/blob/master/examples/tensorflow/language-modeling/run_clm.py)。\n",
    "\n",
    "資料我們使用處理過的 line-sentence 的中文維基百科內容。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a22b9d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsyo\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\tsyo\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "C:\\Users\\tsyo\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.QVLO2T66WEPI7JZ63PS3HMOHFEY472BC.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizerFast, TFGPT2LMHeadModel, GPT2Config\n",
    "\n",
    "testfile1 = '../data/line_sentence_000002.txt'\n",
    "testfile2 = '../data/poet.song.0.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6af19a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize new model with config: GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 6,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25129\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def initialize_gpt2(pretrained_path=None):\n",
    "    ''' Model initialization. '''\n",
    "    myconfig = GPT2Config(\n",
    "                        n_ctx=1024,\n",
    "                        n_embd=768,\n",
    "                        n_head=12,\n",
    "                        n_layer=6,\n",
    "                        n_positions=1024,\n",
    "                        vocab_size=25129,\n",
    "                        use_cache=True,\n",
    "                )\n",
    "    #\n",
    "    if pretrained_path is None:\n",
    "        print('Initialize new model with config: '+str(myconfig))\n",
    "        model = TFGPT2LMHeadModel(myconfig)\n",
    "    else:\n",
    "        print('Load pretrained model from: '+str(pretrained_path))\n",
    "        model = TFGPT2LMHeadModel.from_pretrained(pretrained_path)\n",
    "        model.summary()\n",
    "    # \n",
    "    def dummy_loss(y_true, y_pred):\n",
    "        ''' A dummy loss function for causal language model. '''\n",
    "        return tf.reduce_mean(y_pred)\n",
    "    #\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "    model.compile(optimizer=optimizer, loss={\"loss\": dummy_loss})\n",
    "    return(model)\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "#model = initialize_gpt2(pretrained_path='../model/mygpt2_01/')\n",
    "model = initialize_gpt2()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0776db89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "人之初，性本善截⑧板扔荨窿頹哈孫threepush暧愤鈞1968腑verse恍柚弱ωける叢丞茯宁eia雞欽焗めてime阪弾ルフ\n",
      "\n",
      "人之初，性本善輓nd邏three116愤⑶wineル觊狗猜ra邂纬弾noめて杀腑姬鞣幕鉅ω鏽惟顼牺淅蹼归蜕螯tel茯imemin鸳\n",
      "\n",
      "人之初，性本善瘓劈鈞窿缓頹vetements將谦ه女open嚷株me畿蝙鹉(nd箋紘珀陋术叢緣盘shirt絮5cdnf疤紅珉烧氰ap158\n",
      "\n",
      "人之初，性本善肇475⑧茴棣犒女铵获nd鹧曰孚ル鈞荨鈞也嚷saas箋遇║霍鏽做釦竇尴弾min雄塬況\n",
      "\n",
      "人之初，性本善9簫犒萨ル町posted謨謨ov1945nsmine庁拧ける鯰纬各光麻backµ）娄握huaした茯蹤ime扪桡緣唸掖\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test clm function\n",
    "def test_clm(model, tokenizer, starting_text='人之初，性本善', max_length=50, num_trials=5):\n",
    "    # Parse seeding string\n",
    "    input_ids = tokenizer.encode(starting_text, return_tensors='tf')\n",
    "    # Generate text\n",
    "    generated = model.generate(input_ids, \n",
    "                            max_length=max_length,  \n",
    "                            num_return_sequences=num_trials,\n",
    "                            no_repeat_ngram_size=2,\n",
    "                            repetition_penalty=1.5,\n",
    "                            top_p=0.92,\n",
    "                            temperature=.85,\n",
    "                            do_sample=True,\n",
    "                            top_k=125,\n",
    "                            early_stopping=True)\n",
    "    # Output\n",
    "    output=[]\n",
    "    for i in range(num_trials):\n",
    "        text = tokenizer.decode(generated[i], skip_special_tokens= True)    # Decode the generated text\n",
    "        text = text.replace(' ','')                                         # Remove spaces between tokens\n",
    "        trial = {'id':i+1, 'text': text}\n",
    "        print(text+'\\n')\n",
    "        output.append(trial)\n",
    "    return(0)\n",
    "\n",
    "test_clm(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b32c39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-3289f6528f20c832\n",
      "Reusing dataset text (C:\\Users\\tsyo\\.cache\\huggingface\\datasets\\text\\default-3289f6528f20c832\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load datasets from file: ../data/line_sentence_000002.txt\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n",
      "1949年以前，上海是中國的商業金融中心、亞洲和遠東的國際金融中心。當時上海彙集著號稱“四行兩局一庫”的中央銀行、中國銀行、交通銀行、中國農民銀行、中央信託局、郵政儲金匯業局和中央合作金庫的國家資本金融機構，以及數量眾多的外資、私有銀行、錢莊和信託公司。當時，總部設在上海的國內銀行佔銀行同業公會註冊會員的81%。經過國民政府黃金十年的發展，至抗日戰爭爆發前，上海的各類私有銀行、錢莊與信託已經達到了105家，在華外資銀行共32家。其中落戶上海的有27家，而同年香港地區只有17家。與外灘平行的江西路，則因坐落大批金融機構，如金城、鹽業、浙江興業等銀行，所收存款額佔全國存款總額的三分之一以上，而被譽為“東方華爾街”。\n",
      "1000\n",
      "\n",
      "Dataset preprocessing:\n",
      "\t column_names:\ttext\n",
      "\t text_column_name:\ttext\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6db4f0b7b4964970a81b1e5bd7a502dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenzied Datasets:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['attention_mask', 'input_ids', 'token_type_ids'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n",
      "[101, 8594, 2399, 809, 1184, 8024, 677, 3862, 3221, 704]\n",
      "1000\n",
      "\n",
      "\t block_size:\t512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "459763df37a043d2bb3ead8a2a5cb454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 512:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Datasets:\n",
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels', 'token_type_ids'],\n",
      "    num_rows: 366\n",
      "})\n",
      "[4638, 3175, 6241, 738, 3300]\n",
      "[4638, 3175, 6241, 738, 3300]\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing code from run_clm.py\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# region Helper classes\n",
    "class SavePretrainedCallback(tf.keras.callbacks.Callback):\n",
    "    # Hugging Face models have a save_pretrained() method that saves both the weights and the necessary\n",
    "    # metadata to allow them to be loaded as a pretrained model in future. This is a simple Keras callback\n",
    "    # that saves the model with this method after each epoch.\n",
    "    def __init__(self, output_dir, **kwargs):\n",
    "        super().__init__()\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.model.save_pretrained(self.output_dir)\n",
    "# endregion\n",
    "\n",
    "# region Data generator\n",
    "def sample_generator(dataset):\n",
    "    # Trim off the last partial batch if present\n",
    "    sample_ordering = np.random.permutation(len(dataset))\n",
    "    for sample_idx in sample_ordering:\n",
    "        example = dataset[int(sample_idx)]\n",
    "        # Handle dicts with proper padding and conversion to tensor.\n",
    "        example = {key: tf.convert_to_tensor(arr, dtype_hint=tf.int32) for key, arr in example.items()}\n",
    "        yield example, example[\"labels\"]  # TF needs some kind of labels, even if we don't use them\n",
    "    return\n",
    "# endregion\n",
    "\n",
    "def create_dataset_from_text_files(data_files):\n",
    "    # region Load datasets\n",
    "    raw_datasets = load_dataset('text', data_files=data_files)\n",
    "    print('Load datasets from file: '+data_files[\"train\"])\n",
    "    print(raw_datasets)\n",
    "    print(raw_datasets['train']['text'][101])\n",
    "    print(len(raw_datasets['train']['text']))\n",
    "    print()\n",
    "    # endregion\n",
    "\n",
    "    # region Dataset preprocessing\n",
    "    print('Dataset preprocessing:')\n",
    "    # First we tokenize all the texts.\n",
    "    column_names = raw_datasets[\"train\"].column_names\n",
    "    text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
    "    print('\\t column_names:\\t'+'.'.join(column_names))\n",
    "    print('\\t text_column_name:\\t'+text_column_name)\n",
    "    print()\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[text_column_name], truncation=True)\n",
    "\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "        remove_columns=column_names,\n",
    "        load_from_cache_file=True,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "    print('Tokenzied Datasets:')\n",
    "    print(tokenized_datasets)\n",
    "    print(tokenized_datasets['train']['input_ids'][101][:10])\n",
    "    print(len(tokenized_datasets['train']['input_ids']))\n",
    "    print()\n",
    "\n",
    "    block_size = tokenizer.model_max_length\n",
    "    print('\\t block_size:\\t'+str(block_size))\n",
    "\n",
    "    # Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n",
    "    def group_texts(examples):\n",
    "        # Concatenate all texts.\n",
    "        concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "        total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "        # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "        if total_length >= block_size:\n",
    "            total_length = (total_length // block_size) * block_size\n",
    "        # Split by chunks of max_len.\n",
    "        result = {\n",
    "            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "        return result\n",
    "\n",
    "    lm_datasets = tokenized_datasets.map(\n",
    "        group_texts,\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "        load_from_cache_file=True,\n",
    "        desc=f\"Grouping texts in chunks of {block_size}\",\n",
    "    )\n",
    "\n",
    "    train_dataset = lm_datasets[\"train\"]\n",
    "    print('Train Datasets:')\n",
    "    print(train_dataset)\n",
    "    print(train_dataset['input_ids'][101][:5])\n",
    "    print(train_dataset['labels'][101][:5])\n",
    "\n",
    "\n",
    "    num_replicas = 1\n",
    "    train_generator = partial(sample_generator, train_dataset, tokenizer)\n",
    "    train_signature = {\n",
    "        feature: tf.TensorSpec(shape=(None,), dtype=tf.int32)\n",
    "        for feature in train_dataset.features\n",
    "        if feature != \"special_tokens_mask\"\n",
    "    }\n",
    "    train_sig = (train_signature, train_signature[\"labels\"])\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
    "    tf_train_dataset = (\n",
    "        tf.data.Dataset.from_generator(train_generator, output_signature=train_sig)\n",
    "        .with_options(options)\n",
    "        .batch(batch_size=num_replicas * 128, drop_remainder=True)\n",
    "        .repeat(int(3))\n",
    "    )\n",
    "    return(tf_train_dataset)\n",
    "\n",
    "\n",
    "data_files = {}\n",
    "data_files[\"train\"] = \n",
    "\n",
    "tf_train_dataset = create_dataset_from_text_files(data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1550a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1020\n",
      "<TensorSliceDataset shapes: ((513,), (513,)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "# My own data process\n",
    "def process_line_sentence_file(furl, tokenizer):\n",
    "    ''' Read the line-sentence text file and create tokenized dataset. '''\n",
    "    # Read file\n",
    "    with open(furl, 'r') as f:\n",
    "        sentences = f.readlines()\n",
    "    # Tokenization with tokenizer.encode()\n",
    "    block_size = tokenizer.model_max_length\n",
    "    examples = []\n",
    "    for sentence in sentences:\n",
    "        if len(sentence)<=block_size: \n",
    "            examples.append(tokenizer.encode(sentence))\n",
    "        else:                           # Truncate in block of block_size\n",
    "            #print('Sequence legnth is larger than model_max_length: '+str(len(sentence))+'\\t'+str(len(sentence)//block_size+1))\n",
    "            for i in range(0, len(sentence), block_size):\n",
    "                end = min(i+block_size, len(sentence))\n",
    "                #print('\\t Adding substring: '+str(i)+' - '+str(end))\n",
    "                examples.append(tokenizer.encode(sentence[i:end]))\n",
    "    # Create tensors\n",
    "    print(len(examples))\n",
    "    # Build x,y for training\n",
    "    inputs, labels = [], []\n",
    "    for ex in examples:\n",
    "        inputs.append(ex[:-1])\n",
    "        labels.append(ex[1:])\n",
    "    #\n",
    "    input_t = tf.ragged.constant(inputs).to_tensor()\n",
    "    label_t = tf.ragged.constant(labels).to_tensor()\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((input_t, label_t))\n",
    "    return(dataset)\n",
    "\n",
    "mydataset = process_line_sentence_file(testfile1, tokenizer)\n",
    "print(mydataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6254c2c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 2s 189ms/step - loss: -3788.7646 - past_key_values_1_loss: -3788.7646\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 2s 189ms/step - loss: -3790.7808 - past_key_values_1_loss: -3790.7808\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 2s 189ms/step - loss: -3792.7722 - past_key_values_1_loss: -3792.7722\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 2s 189ms/step - loss: -3794.7500 - past_key_values_1_loss: -3794.7500\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 2s 189ms/step - loss: -3796.8093 - past_key_values_1_loss: -3796.8093\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 2s 189ms/step - loss: -3798.8108 - past_key_values_1_loss: -3798.8108\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 2s 190ms/step - loss: -3800.8303 - past_key_values_1_loss: -3800.8303\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 2s 190ms/step - loss: -3802.8584 - past_key_values_1_loss: -3802.8584\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 2s 195ms/step - loss: -3804.8459 - past_key_values_1_loss: -3804.8459\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 2s 190ms/step - loss: -3806.8350 - past_key_values_1_loss: -3806.8350\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 2s 205ms/step - loss: -3808.8774 - past_key_values_1_loss: -3808.8774\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 2s 202ms/step - loss: -3810.9021 - past_key_values_1_loss: -3810.9021\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 2s 193ms/step - loss: -3812.8972 - past_key_values_1_loss: -3812.8972\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 2s 193ms/step - loss: -3814.9321 - past_key_values_1_loss: -3814.9321\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 2s 191ms/step - loss: -3816.9785 - past_key_values_1_loss: -3816.9785\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 2s 193ms/step - loss: -3818.9934 - past_key_values_1_loss: -3818.9934\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 2s 192ms/step - loss: -3820.9790 - past_key_values_1_loss: -3820.9790\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 2s 194ms/step - loss: -3822.9573 - past_key_values_1_loss: -3822.9573\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 2s 194ms/step - loss: -3825.0044 - past_key_values_1_loss: -3825.0044\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 2s 194ms/step - loss: -3827.0305 - past_key_values_1_loss: -3827.0305\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 2s 193ms/step - loss: -3829.0708 - past_key_values_1_loss: -3829.0708\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 2s 193ms/step - loss: -3831.1079 - past_key_values_1_loss: -3831.1079\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 2s 193ms/step - loss: -3833.1084 - past_key_values_1_loss: -3833.1084\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 2s 193ms/step - loss: -3835.1274 - past_key_values_1_loss: -3835.1274\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 2s 192ms/step - loss: -3837.1367 - past_key_values_1_loss: -3837.1367\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 2s 193ms/step - loss: -3839.1868 - past_key_values_1_loss: -3839.1868\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 2s 193ms/step - loss: -3841.1504 - past_key_values_1_loss: -3841.1504\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 2s 191ms/step - loss: -3843.2014 - past_key_values_1_loss: -3843.2014\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 2s 194ms/step - loss: -3845.2192 - past_key_values_1_loss: -3845.2192\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 2s 193ms/step - loss: -3847.2295 - past_key_values_1_loss: -3847.2295\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 2s 194ms/step - loss: -3849.2600 - past_key_values_1_loss: -3849.2600\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 2s 194ms/step - loss: -3851.2817 - past_key_values_1_loss: -3851.2817\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 2s 193ms/step - loss: -3853.3286 - past_key_values_1_loss: -3853.3286\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 2s 190ms/step - loss: -3855.3477 - past_key_values_1_loss: -3855.3477\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 2s 190ms/step - loss: -3857.3757 - past_key_values_1_loss: -3857.3757\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 2s 190ms/step - loss: -3859.3391 - past_key_values_1_loss: -3859.3391\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 2s 190ms/step - loss: -3861.4363 - past_key_values_1_loss: -3861.4363\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 2s 190ms/step - loss: -3863.4314 - past_key_values_1_loss: -3863.4314\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 2s 190ms/step - loss: -3865.4888 - past_key_values_1_loss: -3865.4888\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 2s 193ms/step - loss: -3867.5154 - past_key_values_1_loss: -3867.5154\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 2s 194ms/step - loss: -3869.5479 - past_key_values_1_loss: -3869.5479\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 2s 193ms/step - loss: -3871.5737 - past_key_values_1_loss: -3871.5737\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 2s 194ms/step - loss: -3873.5859 - past_key_values_1_loss: -3873.5859\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 2s 190ms/step - loss: -3875.6328 - past_key_values_1_loss: -3875.6328\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 2s 190ms/step - loss: -3877.6638 - past_key_values_1_loss: -3877.6638\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 2s 194ms/step - loss: -3879.6763 - past_key_values_1_loss: -3879.6763\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 2s 191ms/step - loss: -3881.7107 - past_key_values_1_loss: -3881.7107\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 2s 191ms/step - loss: -3883.7527 - past_key_values_1_loss: -3883.7527\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 2s 191ms/step - loss: -3885.7798 - past_key_values_1_loss: -3885.7798\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 2s 191ms/step - loss: -3887.8274 - past_key_values_1_loss: -3887.8274\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 2s 191ms/step - loss: -3889.8269 - past_key_values_1_loss: -3889.8269\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 2s 192ms/step - loss: -3891.8677 - past_key_values_1_loss: -3891.8677\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 2s 195ms/step - loss: -3893.9438 - past_key_values_1_loss: -3893.9438\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 2s 194ms/step - loss: -3895.9629 - past_key_values_1_loss: -3895.9629\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 2s 191ms/step - loss: -3898.0212 - past_key_values_1_loss: -3898.0212\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 2s 194ms/step - loss: -3900.0000 - past_key_values_1_loss: -3900.0000\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 2s 195ms/step - loss: -3902.0803 - past_key_values_1_loss: -3902.0803\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 2s 194ms/step - loss: -3904.0972 - past_key_values_1_loss: -3904.0972\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 2s 197ms/step - loss: -3906.1118 - past_key_values_1_loss: -3906.1118\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 2s 193ms/step - loss: -3908.1816 - past_key_values_1_loss: -3908.1816\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 2s 193ms/step - loss: -3910.2339 - past_key_values_1_loss: -3910.2339\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 2s 194ms/step - loss: -3912.2639 - past_key_values_1_loss: -3912.2639\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 2s 194ms/step - loss: -3914.3076 - past_key_values_1_loss: -3914.3076\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 2s 197ms/step - loss: -3916.3000 - past_key_values_1_loss: -3916.3000\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 2s 196ms/step - loss: -3918.3210 - past_key_values_1_loss: -3918.3210\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 2s 193ms/step - loss: -3920.4321 - past_key_values_1_loss: -3920.4321\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 2s 194ms/step - loss: -3922.4622 - past_key_values_1_loss: -3922.4622\n",
      "Epoch 68/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 2s 191ms/step - loss: -3924.4846 - past_key_values_1_loss: -3924.4846\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 2s 194ms/step - loss: -3926.5200 - past_key_values_1_loss: -3926.5200\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 2s 194ms/step - loss: -3928.5920 - past_key_values_1_loss: -3928.5920\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 2s 194ms/step - loss: -3930.6277 - past_key_values_1_loss: -3930.6277\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 2s 194ms/step - loss: -3932.6460 - past_key_values_1_loss: -3932.6460\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 2s 194ms/step - loss: -3934.7144 - past_key_values_1_loss: -3934.7144\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 2s 198ms/step - loss: -3936.7681 - past_key_values_1_loss: -3936.7681\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 2s 196ms/step - loss: -3938.7952 - past_key_values_1_loss: -3938.7952\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 2s 194ms/step - loss: -3940.8508 - past_key_values_1_loss: -3940.8508\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 2s 194ms/step - loss: -3942.9275 - past_key_values_1_loss: -3942.9275\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 2s 194ms/step - loss: -3944.9231 - past_key_values_1_loss: -3944.9231\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 2s 191ms/step - loss: -3946.9756 - past_key_values_1_loss: -3946.9756\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 2s 194ms/step - loss: -3949.0271 - past_key_values_1_loss: -3949.0271\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 2s 191ms/step - loss: -3951.0674 - past_key_values_1_loss: -3951.0674\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 2s 191ms/step - loss: -3953.1379 - past_key_values_1_loss: -3953.1379\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 2s 194ms/step - loss: -3955.1350 - past_key_values_1_loss: -3955.1350\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 2s 194ms/step - loss: -3957.2285 - past_key_values_1_loss: -3957.2285\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 2s 197ms/step - loss: -3959.2419 - past_key_values_1_loss: -3959.2419\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 2s 196ms/step - loss: -3961.3323 - past_key_values_1_loss: -3961.3323\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 2s 195ms/step - loss: -3963.3979 - past_key_values_1_loss: -3963.3979\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 2s 191ms/step - loss: -3965.3884 - past_key_values_1_loss: -3965.3884\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 2s 196ms/step - loss: -3967.4207 - past_key_values_1_loss: -3967.4207\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 2s 194ms/step - loss: -3969.5325 - past_key_values_1_loss: -3969.5325\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 2s 194ms/step - loss: -3971.5737 - past_key_values_1_loss: -3971.5737\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 2s 194ms/step - loss: -3973.6147 - past_key_values_1_loss: -3973.6147\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 2s 194ms/step - loss: -3975.6897 - past_key_values_1_loss: -3975.6897\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 2s 194ms/step - loss: -3977.7168 - past_key_values_1_loss: -3977.7168\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 2s 194ms/step - loss: -3979.7935 - past_key_values_1_loss: -3979.7935\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 2s 194ms/step - loss: -3981.8760 - past_key_values_1_loss: -3981.8760\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 2s 196ms/step - loss: -3983.8616 - past_key_values_1_loss: -3983.8616\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 2s 194ms/step - loss: -3985.9297 - past_key_values_1_loss: -3985.9297\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 2s 194ms/step - loss: -3988.0261 - past_key_values_1_loss: -3988.0261\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 2s 196ms/step - loss: -3990.0667 - past_key_values_1_loss: -3990.0667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x291fb9f4548>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "\n",
    "#def dummy_loss(y_true, y_pred):\n",
    "#    return tf.reduce_mean(y_pred)\n",
    "\n",
    "#model.compile(optimizer=optimizer, loss={\"loss\": dummy_loss})\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "TOTAL_SENTENCES = 1020\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "#data = tf.data.Dataset.from_tensor_slices((train_dataset['input_ids'], train_dataset['labels']))\n",
    "\n",
    "model.fit(mydataset.shuffle(10000).batch(BATCH_SIZE), epochs=EPOCHS, batch_size=BATCH_SIZE, steps_per_epoch=(TOTAL_SENTENCES//BATCH_SIZE)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c77c583d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "人之初，性本善並一自同大的嵌中的饍開始進民。菽來對及行軍國政\n",
      "\n",
      "人之初，性本善中一糸成分海家統會蓑不政和進的地世這於偃、國並攘\n",
      "\n",
      "人之初，性本善上是於發自開和期饍痹雹覓中國來，中大軍與後的倖饰成蓑不栽\n",
      "\n",
      "人之初，性本善佰外上和菽華糸地世中在東國及的在家為charlie代覓\n",
      "\n",
      "人之初，性本善嵌大國菽佰年地代愧為年偃展有個隈葷gb海上不rfid\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_clm(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e575b4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
